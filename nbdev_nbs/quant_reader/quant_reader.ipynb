{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: utils.html\n",
    "title: Paths\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp quant_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import pathlib\n",
    "if \"__file__\" in globals():#only run in the translated python file, as __file__ is not defined with ipython\n",
    "    INTABLE_CONFIG = os.path.join(pathlib.Path(__file__).parent.absolute(), \"../alphabase/constants/const_files/intable_config.yaml\") #the yaml config is located one directory below the python library files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformatting and transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def invert_dictionary(my_map):\n",
    "    inv_map = {}\n",
    "    for k, v in my_map.items():\n",
    "        inv_map[v] = inv_map.get(v, []) + [k]\n",
    "    return inv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def add_mq_protein_group_ids_if_applicable_and_obtain_annotated_file(mq_file, input_type_to_use ,mq_protein_group_file, columns_to_add):\n",
    "    try:\n",
    "        input_type = _get_input_type(mq_file, input_type_to_use)\n",
    "        if (\"maxquant_evidence\" in input_type or \"maxquant_peptides\" in input_type) and (\"aq_reformat\" not in mq_file):\n",
    "            if mq_protein_group_file is None:\n",
    "                print(\"You provided a MaxQuant peptide or evidence file as input. To have the identical ProteinGroups as in the MaxQuant analysis, please provide the ProteinGroups.txt file as well.\")\n",
    "                return mq_file\n",
    "            else:\n",
    "                mq_df = load_input_file_and_de_duplicate_if_evidence(mq_file, input_type, columns_to_add)\n",
    "                id_column = determine_id_column_from_input_df(mq_df)\n",
    "                id2protein_df = create_id_to_protein_df(mq_protein_group_file, id_column)\n",
    "                annotated_mq_df = annotate_mq_df(mq_df, id2protein_df, id_column)\n",
    "                annotated_mq_filename = f\"{mq_file}.protgroup_annotated.tsv\"\n",
    "                save_annotated_mq_df(annotated_mq_df, annotated_mq_filename)\n",
    "                return annotated_mq_filename\n",
    "        else:\n",
    "            return mq_file\n",
    "    except:\n",
    "        return mq_file\n",
    "\n",
    "\n",
    "def _get_input_type(mq_file ,input_type_to_use):\n",
    "    if input_type_to_use is not None:\n",
    "        return input_type_to_use\n",
    "    else:\n",
    "        return get_input_type_and_config_dict(mq_file)[0]\n",
    "    \n",
    "\n",
    "def load_input_file_and_de_duplicate_if_evidence(input_file, input_type, columns_to_add):\n",
    "    input_df = pd.read_csv(input_file, sep = \"\\t\")\n",
    "    if \"maxquant_evidence\" in input_type:\n",
    "        subset_columns = ['id','Sequence','Modified sequence', 'Experiment','Charge', 'Raw file', 'Gene names', 'Intensity', 'Reverse', 'Potential contaminant'] + columns_to_add\n",
    "        columns_to_group_by = ['Sequence','Modified sequence', 'Experiment','Charge', 'Raw file']\n",
    "        input_df = input_df[subset_columns].set_index(columns_to_group_by)\n",
    "        input_df_grouped = input_df.groupby(columns_to_group_by).Intensity.max()\n",
    "        input_df_no_intensities = input_df.drop(columns=[\"Intensity\"])\n",
    "\n",
    "        input_df = input_df_no_intensities.merge(input_df_grouped, how= 'right', left_index=True, right_index=True).reset_index()\n",
    "        input_df = input_df.drop_duplicates(subset=columns_to_group_by)\n",
    "\n",
    "    return input_df\n",
    "\n",
    "def create_id_to_protein_df(mq_protein_group_file, id_column):    \n",
    "    id_mapping_df = pd.read_csv(mq_protein_group_file, sep = \"\\t\", usecols=[\"Protein IDs\", id_column])\n",
    "    #apply lambda function to id column to split it into a list of ids\n",
    "    id_mapping_df[id_column] = id_mapping_df[id_column].apply(lambda x: x.split(\";\"))\n",
    "    #explode the id column\n",
    "    id_mapping_df = id_mapping_df.explode(id_column) #https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows\n",
    "    return id_mapping_df\n",
    "\n",
    "\n",
    "def determine_id_column_from_input_df(input_df):\n",
    "    input_file_columns = input_df.columns\n",
    "    num_cols_starting_w_intensity = sum([x.startswith(\"Intensity \") for x in input_file_columns])\n",
    "    if num_cols_starting_w_intensity>0:\n",
    "        return \"Peptide IDs\"\n",
    "    else:\n",
    "        return \"Evidence IDs\"\n",
    "\n",
    "\n",
    "def annotate_mq_df(mq_df, id2protein_df, id_column):\n",
    "    #set dtype of id to string\n",
    "    mq_df[\"id\"] = mq_df[\"id\"].astype(str)\n",
    "    id2protein_df = remove_ids_not_occurring_in_mq_df(id2protein_df, mq_df, id_column)\n",
    "    return mq_df.merge(id2protein_df, how = \"right\",  left_on = \"id\", right_on = id_column, suffixes=('', '_y'))\n",
    "\n",
    "def remove_ids_not_occurring_in_mq_df(id2protein_df, mq_df, id_column):\n",
    "    mq_df_ids = set(mq_df[\"id\"])\n",
    "    id2protein_df = id2protein_df[id2protein_df[id_column].isin(mq_df_ids)]\n",
    "    return id2protein_df\n",
    "\n",
    "def save_annotated_mq_df(annotated_mq_df, annotated_mq_file):\n",
    "    annotated_mq_df.to_csv(annotated_mq_file, sep = \"\\t\", index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from distutils.command.config import config\n",
    "\n",
    "\n",
    "def add_columns_to_lfq_results_table(lfq_results_df, input_file, columns_to_add):\n",
    "    input_type, config_dict, _ = get_input_type_and_config_dict(input_file)\n",
    "\n",
    "    input_file = clean_input_filename_if_necessary(input_file)\n",
    "\n",
    "    protein_column_input_table = get_protein_column_input_table(config_dict)\n",
    "    standard_columns_for_input_type = get_standard_columns_for_input_type(input_type)\n",
    "\n",
    "    all_columns = columns_to_add + [protein_column_input_table] + standard_columns_for_input_type\n",
    "    all_columns = filter_columns_to_existing_columns(all_columns, input_file)\n",
    "\n",
    "    input_df = pd.read_csv(input_file, sep=\"\\t\", usecols=all_columns).drop_duplicates(subset=protein_column_input_table)\n",
    "    lfq_results_df = lfq_results_df[[x is not None for x in lfq_results_df['protein']]]\n",
    "\n",
    "    length_before = len(lfq_results_df.index)\n",
    "    lfq_results_df_appended = pd.merge(lfq_results_df, input_df, left_on='protein', right_on=protein_column_input_table, how='left')\n",
    "    length_after = len(lfq_results_df_appended.index)\n",
    "\n",
    "    lfq_results_df_appended = lfq_results_df_appended.set_index('protein')\n",
    "    \n",
    "\n",
    "    assert length_before == length_after\n",
    "    return lfq_results_df_appended\n",
    "\n",
    "def clean_input_filename_if_necessary(input_file):\n",
    "    if \"aq_reformat.tsv\" in input_file:\n",
    "        input_file = get_original_file_from_aq_reformat(input_file)\n",
    "    return input_file\n",
    "\n",
    "def get_protein_column_input_table(config_dict):\n",
    "    return config_dict[\"protein_cols\"][0]\n",
    "\n",
    "def get_standard_columns_for_input_type(input_type):\n",
    "    \n",
    "    if 'maxquant' in input_type:\n",
    "        return [\"Gene names\"]\n",
    "    elif 'diann' in input_type:\n",
    "        return [\"Protein.Names\", \"Genes\"]\n",
    "    elif 'spectronaut' in input_type:\n",
    "        return ['PG.Genes']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def filter_columns_to_existing_columns(columns, input_file):\n",
    "    existing_columns =  pd.read_csv(input_file, sep='\\t', nrows=1).columns\n",
    "    return [x for x in columns if x in existing_columns]\n",
    "\n",
    "\n",
    "\n",
    "#function that shows the differing rows between two dataframes\n",
    "def show_diff(df1, df2):\n",
    "    return df1.merge(df2, indicator=True, how='outer').loc[lambda x : x['_merge']!='both']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def write_chunk_to_file(chunk, filepath ,write_header):\n",
    "    \"\"\"write chunk of pandas dataframe to a file\"\"\"\n",
    "    chunk.to_csv(filepath, header=write_header, mode='a', sep = \"\\t\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "def index_and_log_transform_input_df(data_df):\n",
    "    data_df = data_df.set_index([\"protein\", \"ion\"])\n",
    "    return np.log2(data_df.replace(0, np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_allnan_rows_input_df(data_df):\n",
    "    return data_df.dropna(axis = 0, how = 'all')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Parsers\n",
    "The directlfq pipeline is run using a generic wide-table input format, as specified in the documentation. The following parsers convert long format tables as provided e.g. by Spectronaut or DIA-NN into this generic format. The configuration for the parsers is set by a yaml file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert long format to wide format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse .yaml file\n",
    "The relevant parameters for reading and reformatting the long table are stored in the \"intable_config.yaml\" file. The functions below are for reading and reformating the config info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import yaml\n",
    "import itertools\n",
    "\n",
    "def get_relevant_columns(protein_cols, ion_cols, sample_ID, quant_ID, filter_dict):\n",
    "    filtcols = []\n",
    "    for filtconf in filter_dict.values():\n",
    "        filtcols.append(filtconf.get('param'))\n",
    "    relevant_cols = protein_cols + ion_cols + [sample_ID] + [quant_ID] + filtcols\n",
    "    relevant_cols = list(set(relevant_cols)) # to remove possible redudancies\n",
    "    return relevant_cols\n",
    "\n",
    "\n",
    "def get_relevant_columns_config_dict(config_typedict):\n",
    "    filtcols = []\n",
    "    dict_ioncols = []\n",
    "    for filtconf in config_typedict.get('filters', {}).values():\n",
    "        filtcols.append(filtconf.get('param'))\n",
    "\n",
    "    if 'ion_hierarchy' in config_typedict.keys():\n",
    "        for headr in config_typedict.get('ion_hierarchy').values():\n",
    "            ioncols = list(itertools.chain.from_iterable(headr.get(\"mapping\").values()))\n",
    "            dict_ioncols.extend(ioncols)\n",
    "\n",
    "    quant_ids = get_quant_ids_from_config_dict(config_typedict)\n",
    "    sample_ids = get_sample_ids_from_config_dict(config_typedict)\n",
    "    channel_ids = get_channel_ids_from_config_dict(config_typedict)\n",
    "    relevant_cols = config_typedict.get(\"protein_cols\") + config_typedict.get(\"ion_cols\", []) + sample_ids + quant_ids + filtcols + dict_ioncols + channel_ids\n",
    "    relevant_cols = list(set(relevant_cols)) # to remove possible redudancies\n",
    "    return relevant_cols\n",
    "\n",
    "def get_quant_ids_from_config_dict(config_typedict):\n",
    "    quantID = config_typedict.get(\"quant_ID\")\n",
    "    if type(quantID) ==type(\"string\"):\n",
    "        return [config_typedict.get(\"quant_ID\")]\n",
    "    if quantID == None:\n",
    "        return[]\n",
    "    else:\n",
    "        return list(config_typedict.get(\"quant_ID\").values())\n",
    "\n",
    "def get_sample_ids_from_config_dict(config_typedict):\n",
    "    sampleID = config_typedict.get(\"sample_ID\")\n",
    "    if type(sampleID) ==type(\"string\"):\n",
    "        return [config_typedict.get(\"sample_ID\")]\n",
    "    if sampleID == None:\n",
    "        return []\n",
    "    else:\n",
    "        return config_typedict.get(\"sample_ID\")\n",
    "\n",
    "def get_channel_ids_from_config_dict(config_typedict):\n",
    "    return config_typedict.get(\"channel_ID\", [])\n",
    "\n",
    "\n",
    "\n",
    "def load_config(config_yaml):\n",
    "    with open(config_yaml, 'r') as stream:\n",
    "        config_all = yaml.safe_load(stream)\n",
    "    return config_all\n",
    "\n",
    "def get_type2relevant_cols(config_all):\n",
    "    type2relcols = {}\n",
    "    for type in config_all.keys():\n",
    "        config_typedict = config_all.get(type)\n",
    "        relevant_cols = get_relevant_columns_config_dict(config_typedict)\n",
    "        type2relcols[type] = relevant_cols\n",
    "    return type2relcols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def filter_input(filter_dict, input):\n",
    "    if filter_dict == None:\n",
    "        return input\n",
    "    for filtname,filterconf in filter_dict.items():\n",
    "        param = filterconf.get('param')\n",
    "        comparator = filterconf.get('comparator')\n",
    "        value = filterconf.get('value')\n",
    "\n",
    "        if comparator not in [\">\",\">=\", \"<\", \"<=\", \"==\", \"!=\"]:\n",
    "            raise TypeError(f\"cannot identify the filter comparator of {filtname} given in the longtable config yaml!\")\n",
    "\n",
    "        if comparator==\"==\":\n",
    "            input = input[input[param] ==value]\n",
    "            continue\n",
    "        try:\n",
    "            input = input.astype({f\"{param}\" : \"float\"})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if comparator==\">\":\n",
    "            input = input[input[param].astype(type(value)) >value]\n",
    "\n",
    "        if comparator==\">=\":\n",
    "            input = input[input[param].astype(type(value)) >=value]\n",
    "\n",
    "        if comparator==\"<\":\n",
    "            input = input[input[param].astype(type(value)) <value]\n",
    "\n",
    "        if comparator==\"<=\":\n",
    "            input = input[input[param].astype(type(value)) <=value]\n",
    "\n",
    "        if comparator==\"!=\":\n",
    "            input = input[input[param].astype(type(value)) !=value]\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def merge_protein_and_ion_cols(input_df, config_dict):\n",
    "    protein_cols =  config_dict.get(\"protein_cols\")\n",
    "    ion_cols = config_dict.get(\"ion_cols\")\n",
    "    input_df['protein'] = input_df.loc[:, protein_cols].astype('string').sum(axis=1)\n",
    "    input_df['ion'] = input_df.loc[:, ion_cols].astype('string').sum(axis=1)\n",
    "    input_df = input_df.rename(columns = {config_dict.get('quant_ID') : \"quant_val\"})\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "def merge_protein_cols_and_ion_dict(input_df, config_dict):\n",
    "    \"\"\"[summary]\n",
    "    \n",
    "    Args:\n",
    "        input_df ([pandas dataframe]): longtable containing peptide intensity data\n",
    "        confid_dict ([dict[String[]]]): nested dict containing the parse information. derived from yaml file\n",
    "\n",
    "    Returns:\n",
    "        pandas dataframe: longtable with newly assigned \"protein\" and \"ion\" columns\n",
    "    \"\"\"\n",
    "    protein_cols = config_dict.get(\"protein_cols\")\n",
    "    ion_hierarchy = config_dict.get(\"ion_hierarchy\")\n",
    "    splitcol2sep = config_dict.get('split_cols')\n",
    "    quant_id_dict = config_dict.get('quant_ID')\n",
    "\n",
    "    ion_dfs = []\n",
    "    input_df['protein'] = input_df.loc[:, protein_cols].astype('string').sum(axis=1)\n",
    "\n",
    "    input_df = input_df.drop(columns = [x for x in protein_cols if x!='protein'])\n",
    "    for hierarchy_type in ion_hierarchy.keys():\n",
    "        df_subset = input_df.copy()\n",
    "        ion_hierarchy_local = ion_hierarchy.get(hierarchy_type).get(\"order\")\n",
    "        ion_headers_merged, ion_headers_grouped = get_ionname_columns(ion_hierarchy.get(hierarchy_type).get(\"mapping\"), ion_hierarchy_local) #ion headers merged is just a helper to select all relevant rows, ionheaders grouped contains the sets of ionstrings to be merged into a list eg [[SEQ, MOD], [CH]]\n",
    "        quant_columns = get_quantitative_columns(df_subset, hierarchy_type, config_dict, ion_headers_merged)\n",
    "        headers = list(set(ion_headers_merged + quant_columns + ['protein']))\n",
    "        if \"sample_ID\" in config_dict.keys():\n",
    "            headers+=[config_dict.get(\"sample_ID\")]\n",
    "        df_subset = df_subset[headers].drop_duplicates()\n",
    "\n",
    "        if splitcol2sep is not None:\n",
    "            if quant_columns[0] in splitcol2sep.keys(): #in the case that quantitative values are stored grouped in one column (e.g. msiso1,msiso2,msiso3, etc.), reformat accordingly\n",
    "                df_subset = split_extend_df(df_subset, splitcol2sep)\n",
    "            ion_headers_grouped = adapt_headers_on_extended_df(ion_headers_grouped, splitcol2sep)\n",
    "\n",
    "        #df_subset = df_subset.set_index(quant_columns)\n",
    "\n",
    "        df_subset = add_merged_ionnames(df_subset, ion_hierarchy_local, ion_headers_grouped, quant_id_dict, hierarchy_type)\n",
    "        ion_dfs.append(df_subset)\n",
    "    input_df = pd.concat(ion_dfs, ignore_index=True)\n",
    "    return input_df\n",
    "\n",
    "\n",
    "def get_quantitative_columns(input_df, hierarchy_type, config_dict, ion_headers_merged):\n",
    "    naming_columns = ion_headers_merged + ['protein']\n",
    "    if config_dict.get(\"format\") == 'longtable':\n",
    "        quantcol = config_dict.get(\"quant_ID\").get(hierarchy_type)\n",
    "        return [quantcol]\n",
    "\n",
    "    if config_dict.get(\"format\") == 'widetable':\n",
    "        quantcolumn_candidates = [x for x in input_df.columns if x not in naming_columns]\n",
    "        if \"quant_prefix\" in config_dict.keys():\n",
    "            return [x for x in quantcolumn_candidates if x.startswith(config_dict.get(\"quant_prefix\"))] # in the case that the quantitative columns have a prefix (like \"Intensity \" in MQ peptides.txt), only columns with the prefix are filtered\n",
    "        else:\n",
    "            return quantcolumn_candidates #in this case, we assume that all non-ionname/proteinname columns are quantitative columns\n",
    "\n",
    "\n",
    "def get_ionname_columns(ion_dict, ion_hierarchy_local):\n",
    "    ion_headers_merged = []\n",
    "    ion_headers_grouped = []\n",
    "    for lvl in ion_hierarchy_local:\n",
    "        vals = ion_dict.get(lvl)\n",
    "        ion_headers_merged.extend(vals)\n",
    "        ion_headers_grouped.append(vals)\n",
    "    return ion_headers_merged, ion_headers_grouped\n",
    "\n",
    "\n",
    "def adapt_headers_on_extended_df(ion_headers_grouped, splitcol2sep):\n",
    "    #in the case that one column has been split, we need to designate the \"naming\" column\n",
    "    ion_headers_grouped_copy = copy.deepcopy(ion_headers_grouped)\n",
    "    for vals in ion_headers_grouped_copy:\n",
    "        if splitcol2sep is not None:\n",
    "            for idx in range(len(vals)):\n",
    "                if vals[idx] in splitcol2sep.keys():\n",
    "                    vals[idx] = vals[idx] + \"_idxs\"\n",
    "    return ion_headers_grouped_copy\n",
    "\n",
    "def split_extend_df(input_df, splitcol2sep, value_threshold=10):\n",
    "    \"\"\"reformats data that is stored in a condensed way in a single column. For example isotope1_intensity;isotope2_intensity etc. in Spectronaut\n",
    "\n",
    "    Args:\n",
    "        input_df ([type]): [description]\n",
    "        splitcol2sep ([type]): [description]\n",
    "        value_threshold([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        Pandas Dataframe: Pandas dataframe with the condensed items expanded to long format\n",
    "    \"\"\"\n",
    "    if splitcol2sep==None:\n",
    "        return input_df\n",
    "\n",
    "    for split_col, separator in splitcol2sep.items():\n",
    "        idx_name = f\"{split_col}_idxs\"\n",
    "        split_col_series = input_df[split_col].str.split(separator)\n",
    "        input_df = input_df.drop(columns = [split_col])\n",
    "\n",
    "        input_df[idx_name] = [list(range(len(x))) for x in split_col_series]\n",
    "        exploded_input = input_df.explode(idx_name)\n",
    "        exploded_split_col_series = split_col_series.explode()\n",
    "\n",
    "        exploded_input[split_col] = exploded_split_col_series.replace('', 0) #the column with the intensities has to come after to column with the idxs\n",
    "\n",
    "        exploded_input = exploded_input.astype({split_col: float})\n",
    "        exploded_input = exploded_input[exploded_input[split_col]>value_threshold]\n",
    "        #exploded_input = exploded_input.rename(columns = {'var1': split_col})\n",
    "    return exploded_input\n",
    "\n",
    "\n",
    "\n",
    "def add_merged_ionnames(df_subset, ion_hierarchy_local, ion_headers_grouped, quant_id_dict, hierarchy_type):\n",
    "    \"\"\"puts together the hierarchical ion names as a column in a given input dataframe\"\"\"\n",
    "    all_ion_headers = list(itertools.chain.from_iterable(ion_headers_grouped))\n",
    "    columns_to_index = [x for x in df_subset.columns if x not in all_ion_headers]\n",
    "    df_subset = df_subset.set_index(columns_to_index)\n",
    "\n",
    "    rows = df_subset[all_ion_headers].to_numpy()\n",
    "    ions = []\n",
    "\n",
    "    for row in rows: #iterate through dataframe\n",
    "        count = 0\n",
    "        ionstring = \"\"\n",
    "        for lvl_idx in range(len(ion_hierarchy_local)):\n",
    "            ionstring += f\"{ion_hierarchy_local[lvl_idx]}\"\n",
    "            for sublvl in ion_headers_grouped[lvl_idx]:\n",
    "                ionstring+= f\"_{row[count]}_\"\n",
    "                count+=1\n",
    "        ions.append(ionstring)\n",
    "    df_subset['ion'] = ions\n",
    "    df_subset = df_subset.reset_index()\n",
    "    if quant_id_dict!= None:\n",
    "        df_subset = df_subset.rename(columns = {quant_id_dict.get(hierarchy_type) : \"quant_val\"})\n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os.path\n",
    "def reformat_and_write_longtable_according_to_config(input_file, outfile_name, config_dict_for_type, sep = \"\\t\",decimal = \".\", enforce_largefile_processing = False, chunksize =1000_000):\n",
    "    \"\"\"Reshape a long format proteomics results table (e.g. Spectronaut or DIA-NN) to a wide format table.\n",
    "    :param file input_file: long format proteomic results table\n",
    "    :param string input_type: the configuration key stored in the config file (e.g. \"diann_precursor\")\n",
    "    \"\"\"\n",
    "    filesize = os.path.getsize(input_file)/(1024**3) #size in gigabyte\n",
    "    file_is_large = (filesize>10 and str(input_file).endswith(\".zip\")) or filesize>50 or enforce_largefile_processing\n",
    "\n",
    "    if file_is_large:\n",
    "        tmpfile_large = f\"{input_file}.tmp.longformat.columnfilt.tsv\" #only needed when file is large\n",
    "        #remove potential leftovers from previous processings\n",
    "        if os.path.exists(tmpfile_large):\n",
    "            os.remove(tmpfile_large)\n",
    "        if os.path.exists(outfile_name):\n",
    "            os.remove(outfile_name)\n",
    "    \n",
    "    relevant_cols = get_relevant_columns_config_dict(config_dict_for_type)\n",
    "    input_df_it = pd.read_csv(input_file, sep = sep, decimal=decimal, usecols = relevant_cols, encoding ='latin1', chunksize = chunksize)\n",
    "    input_df_list = []\n",
    "    header = True\n",
    "    for input_df_subset in input_df_it:\n",
    "        input_df_subset = adapt_subtable(input_df_subset, config_dict_for_type)\n",
    "        if file_is_large:\n",
    "            write_chunk_to_file(input_df_subset,tmpfile_large, header)\n",
    "        else:\n",
    "            input_df_list.append(input_df_subset)\n",
    "        header = False\n",
    "        \n",
    "    if file_is_large:\n",
    "        process_with_dask(tmpfile_columnfilt=tmpfile_large , outfile_name = outfile_name, config_dict_for_type=config_dict_for_type)\n",
    "    else:\n",
    "        input_df = pd.concat(input_df_list)\n",
    "        input_reshaped = reshape_input_df(input_df, config_dict_for_type)\n",
    "        input_reshaped.to_csv(outfile_name, sep = \"\\t\", index = None)\n",
    "    \n",
    "\n",
    "def adapt_subtable(input_df_subset, config_dict):\n",
    "    input_df_subset = filter_input(config_dict.get(\"filters\", {}), input_df_subset)\n",
    "    if \"ion_hierarchy\" in config_dict.keys():\n",
    "        return merge_protein_cols_and_ion_dict(input_df_subset, config_dict)\n",
    "    else:\n",
    "        return merge_protein_and_ion_cols(input_df_subset, config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import shutil \n",
    "\n",
    "def process_with_dask(*, tmpfile_columnfilt, outfile_name, config_dict_for_type):\n",
    "    df = dd.read_csv(tmpfile_columnfilt, sep = \"\\t\")\n",
    "    allcols = df[config_dict_for_type.get(\"sample_ID\")].drop_duplicates().compute() # the columns of the output table are the sample IDs\n",
    "    allcols = extend_sample_allcolumns_for_plexdia_case(allcols_samples=allcols, config_dict_for_type=config_dict_for_type)\n",
    "    allcols = ['protein', 'ion'] + sorted(allcols)\n",
    "    df = df.set_index('protein')\n",
    "    sorted_filedir = f\"{tmpfile_columnfilt}_sorted\"\n",
    "    df.to_csv(sorted_filedir, sep = \"\\t\")\n",
    "    #now the files are sorted and can be pivoted chunkwise (multiindex pivoting at the moment not possible in dask)\n",
    "    files_dask = glob.glob(f\"{sorted_filedir}/*part\")\n",
    "    header = True\n",
    "    for file in files_dask:\n",
    "        input_df = pd.read_csv(file, sep = \"\\t\")\n",
    "        if len(input_df.index) <2:\n",
    "            continue\n",
    "        input_reshaped = reshape_input_df(input_df, config_dict_for_type)\n",
    "        input_reshaped = sort_and_add_columns(input_reshaped, allcols)\n",
    "        write_chunk_to_file(input_reshaped, outfile_name, header)\n",
    "        header = False\n",
    "    os.remove(tmpfile_columnfilt)\n",
    "    shutil.rmtree(sorted_filedir)\n",
    "\n",
    "def reshape_input_df(input_df, config_dict):\n",
    "    input_df = input_df.astype({'quant_val': 'float'})\n",
    "    input_df = adapt_input_df_columns_in_case_of_plexDIA(input_df=input_df, config_dict_for_type=config_dict)\n",
    "    input_reshaped = pd.pivot_table(input_df, index = ['protein', 'ion'], columns = config_dict.get(\"sample_ID\"), values = 'quant_val', fill_value=0)\n",
    "\n",
    "    input_reshaped = input_reshaped.reset_index()\n",
    "    return input_reshaped\n",
    "\n",
    "\n",
    "def sort_and_add_columns(input_reshaped, allcols):\n",
    "    missing_cols = set(allcols) - set(input_reshaped.columns)\n",
    "    input_reshaped[list(missing_cols)] = 0\n",
    "    input_reshaped = input_reshaped[allcols]\n",
    "    return input_reshaped\n",
    "\n",
    "\n",
    "def extend_sample_allcolumns_for_plexdia_case(allcols_samples, config_dict_for_type):\n",
    "    if is_plexDIA_table(config_dict_for_type):\n",
    "        new_allcols = []\n",
    "        channels = ['mTRAQ-n-0', 'mTRAQ-n-4', 'mTRAQ-n-8']\n",
    "        for channel in channels:\n",
    "            for sample in allcols_samples:\n",
    "                new_allcols.append(merge_channel_and_sample_string(sample, channel))\n",
    "        return new_allcols\n",
    "    else:\n",
    "        return allcols_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#PLEXDIA case\n",
    "\n",
    "def adapt_input_df_columns_in_case_of_plexDIA(input_df,config_dict_for_type):\n",
    "    if is_plexDIA_table(config_dict_for_type):\n",
    "        input_df = extend_sampleID_column_for_plexDIA_case(input_df, config_dict_for_type)\n",
    "        input_df = set_mtraq_reduced_ion_column_into_dataframe(input_df)\n",
    "        return input_df\n",
    "    else:\n",
    "        return input_df\n",
    "\n",
    "\n",
    "def extend_sampleID_column_for_plexDIA_case(input_df,config_dict_for_type):\n",
    "    channels_per_peptide = parse_channel_from_peptide_column(input_df)\n",
    "    return merge_sample_id_and_channels(input_df, channels_per_peptide, config_dict_for_type)\n",
    "\n",
    "\n",
    "def set_mtraq_reduced_ion_column_into_dataframe(input_df):\n",
    "    new_ions = remove_mtraq_modifications_from_ion_ids(input_df['ion'])\n",
    "    input_df['ion'] = new_ions\n",
    "    return input_df\n",
    "\n",
    "def remove_mtraq_modifications_from_ion_ids(ions):\n",
    "    new_ions = []\n",
    "    all_mtraq_tags = [\"(mTRAQ-K-0)\", \"(mTRAQ-K-4)\", \"(mTRAQ-K-8)\", \"(mTRAQ-n-0)\", \"(mTRAQ-n-4)\", \"(mTRAQ-n-8)\"]\n",
    "    for ion in ions:\n",
    "        for tag in all_mtraq_tags:\n",
    "            ion = ion.replace(tag, \"\")\n",
    "        new_ions.append(ion)\n",
    "    return new_ions\n",
    "\n",
    "\n",
    "def is_plexDIA_table(config_dict_for_type):\n",
    "    return config_dict_for_type.get('channel_ID') == ['Channel.0', 'Channel.4', 'Channel.8']\n",
    "\n",
    "\n",
    "import re\n",
    "def parse_channel_from_peptide_column(input_df):\n",
    "    channels = []\n",
    "    for pep in input_df['Modified.Sequence']:\n",
    "        pattern = \"(.*)(\\(mTRAQ-n-.\\))(.*)\"\n",
    "        matched = re.match(pattern, pep)\n",
    "        num_appearances = pep.count(\"mTRAQ-n-\")\n",
    "        if matched and num_appearances==1:\n",
    "            channels.append(matched.group(2))\n",
    "        else:\n",
    "            channels.append(\"NA\")\n",
    "    return channels\n",
    "\n",
    "def merge_sample_id_and_channels(input_df, channels, config_dict_for_type):\n",
    "    sample_id = config_dict_for_type.get(\"sample_ID\")\n",
    "    sample_ids = list(input_df[sample_id])\n",
    "    input_df[sample_id] = [merge_channel_and_sample_string(sample_ids[idx], channels[idx]) for idx in range(len(sample_ids))]\n",
    "    return input_df\n",
    "            \n",
    "def merge_channel_and_sample_string(sample, channel):\n",
    "    return f\"{sample}_{channel}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def test_remove_remove_mtraq_modifications_from_ion_ids():\n",
    "    ions = [\"SEQ_IAVLLAK_MOD_(mTRAQ-n-0)IAVLLAK(mTRAQ-K-0)\", \"SEQ_IAVLLAK_MOD_(mTRAQ-n-0)IAVLLAK(mTRAQ-K-0)_CHARGE_1_FRGION_2_\",  \"SEQ_IAVLLAK_MOD_(mTRAQ-n-0)I(mTRAQ-n-0)AVL(mTRAQ-K-0)LAK(mTRAQ-K-0)_CHARGE_1_FRGION_2_\"]\n",
    "    new_ions = remove_mtraq_modifications_from_ion_ids(ions)\n",
    "    for ion in new_ions:\n",
    "        assert 'mTRAQ' not in ion\n",
    "test_remove_remove_mtraq_modifications_from_ion_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reformat_and_write_wideformat_table(peptides_tsv, outfile_name, config_dict):\n",
    "    input_df = pd.read_csv(peptides_tsv,sep=\"\\t\", encoding ='latin1')\n",
    "    filter_dict = config_dict.get(\"filters\")\n",
    "    protein_cols = config_dict.get(\"protein_cols\")\n",
    "    ion_cols = config_dict.get(\"ion_cols\")\n",
    "    input_df = filter_input(filter_dict, input_df)\n",
    "    #input_df = merge_protein_and_ion_cols(input_df, config_dict)\n",
    "    input_df = merge_protein_cols_and_ion_dict(input_df, config_dict)\n",
    "    if 'quant_prefix' in config_dict.keys():\n",
    "        quant_prefix = config_dict.get('quant_prefix')\n",
    "        headers = ['protein', 'ion'] + list(filter(lambda x: x.startswith(quant_prefix), input_df.columns))\n",
    "        input_df = input_df[headers]\n",
    "        input_df = input_df.rename(columns = lambda x : x.replace(quant_prefix, \"\"))\n",
    "\n",
    "    #input_df = input_df.reset_index()\n",
    "    \n",
    "    input_df.to_csv(outfile_name, sep = '\\t', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_mq_peptides_table(peptides_tsv, pepheader = \"Sequence\", protheader = \"Leading razor protein\"):\n",
    "    peps = pd.read_csv(peptides_tsv,sep=\"\\t\", encoding ='latin1')\n",
    "    peps = peps[peps[\"Reverse\"] != \"+\"]\n",
    "    peps = peps[peps[\"Potential contaminant\"] != \"+\"]\n",
    "    if pepheader != None:\n",
    "        peps = peps.rename(columns = {pepheader : \"ion\"})\n",
    "    if protheader != None:\n",
    "        peps = peps.rename(columns = {protheader: \"protein\"})\n",
    "    headers = ['protein', 'ion'] + list(filter(lambda x: x.startswith(\"Intensity \"), peps.columns))\n",
    "    peps = peps[headers]\n",
    "    peps = peps.rename(columns = lambda x : x.replace(\"Intensity \", \"\"))\n",
    "\n",
    "    return peps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for already processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "def check_for_processed_runs_in_results_folder(results_folder):\n",
    "    contained_condpairs = []\n",
    "    folder_files = os.listdir(results_folder)\n",
    "    result_files = list(filter(lambda x: \"results.tsv\" in x ,folder_files))\n",
    "    for result_file in result_files:\n",
    "        res_name = result_file.replace(\".results.tsv\", \"\")\n",
    "        if ((f\"{res_name}.normed.tsv\" in folder_files) & (f\"{res_name}.results.ions.tsv\" in folder_files)):\n",
    "            contained_condpairs.append(res_name)\n",
    "    return contained_condpairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "def import_data(input_file, input_type_to_use = None, samples_subset = None, results_dir = None):\n",
    "    \"\"\"\n",
    "    Function to import peptide level data. Depending on available columns in the provided file,\n",
    "    the function identifies the type of input used (e.g. Spectronaut, MaxQuant, DIA-NN), reformats if necessary\n",
    "    and returns a generic wide-format dataframe\n",
    "    :param file input_file: quantified peptide/ion -level data\n",
    "    :param file results_folder: the folder where the directlfq outputs are stored\n",
    "    \"\"\"\n",
    "\n",
    "    samples_subset = add_ion_protein_headers_if_applicable(samples_subset)\n",
    "    if \"aq_reformat\" in input_file:\n",
    "        file_to_read = input_file\n",
    "    else:\n",
    "        file_to_read = reformat_and_save_input_file(input_file=input_file, input_type_to_use=input_type_to_use)\n",
    "    \n",
    "    input_reshaped = pd.read_csv(file_to_read, sep = \"\\t\", encoding = 'latin1', usecols=samples_subset)\n",
    "    input_reshaped = input_reshaped.drop_duplicates(subset='ion')\n",
    "    return input_reshaped\n",
    "\n",
    "\n",
    "def reformat_and_save_input_file(input_file, input_type_to_use = None):\n",
    "    \n",
    "    input_type, config_dict_for_type, sep = get_input_type_and_config_dict(input_file, input_type_to_use)\n",
    "    print(f\"using input type {input_type}\")\n",
    "    format = config_dict_for_type.get('format')\n",
    "    outfile_name = f\"{input_file}.{input_type}.aq_reformat.tsv\"\n",
    "\n",
    "    if format == \"longtable\":\n",
    "        reformat_and_write_longtable_according_to_config(input_file, outfile_name,config_dict_for_type, sep = sep)\n",
    "    elif format == \"widetable\":\n",
    "        reformat_and_write_wideformat_table(input_file, outfile_name, config_dict_for_type)\n",
    "    else:\n",
    "        raise Exception('Format not recognized!')\n",
    "    return outfile_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_ion_protein_headers_if_applicable(samples_subset):\n",
    "    if samples_subset is not None:\n",
    "        return samples_subset + [\"ion\", \"protein\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import pathlib\n",
    "\n",
    "def get_input_type_and_config_dict(input_file, input_type_to_use = None):\n",
    "    #parse the type of input (e.g. Spectronaut Fragion+MS1Iso) out of the input file\n",
    "\n",
    "\n",
    "    config_dict = load_config(INTABLE_CONFIG)\n",
    "    type2relevant_columns = get_type2relevant_cols(config_dict)\n",
    "\n",
    "    if \"aq_reformat.tsv\" in input_file:\n",
    "        input_file = get_original_file_from_aq_reformat(input_file)\n",
    "\n",
    "    filename = str(input_file)\n",
    "    if '.csv' in filename:\n",
    "        sep=','\n",
    "    if '.tsv' in filename:\n",
    "        sep='\\t'\n",
    "    if '.txt' in filename:\n",
    "        sep='\\t'\n",
    "\n",
    "    if 'sep' not in locals():\n",
    "        raise TypeError(f\"neither of the file extensions (.tsv, .csv, .txt) detected for file {input_file}! Your filename has to contain one of these extensions. Please modify your file name accordingly.\")\n",
    "\n",
    "\n",
    "\n",
    "    uploaded_data_columns = set(pd.read_csv(input_file, sep=sep, nrows=1, encoding ='latin1').columns)\n",
    "\n",
    "    for input_type in type2relevant_columns.keys():\n",
    "        if (input_type_to_use is not None) and (input_type!=input_type_to_use):\n",
    "            continue\n",
    "        relevant_columns = type2relevant_columns.get(input_type)\n",
    "        relevant_columns = [x for x in relevant_columns if x] #filter None values\n",
    "        if set(relevant_columns).issubset(uploaded_data_columns):\n",
    "            config_dict_type =  config_dict.get(input_type)\n",
    "            return input_type, config_dict_type, sep\n",
    "    raise TypeError(\"format not specified in intable_config.yaml!\")\n",
    "\n",
    "import re\n",
    "def get_original_file_from_aq_reformat(input_file):\n",
    "    matched = re.match(\"(.*)(\\..*\\.)(aq_reformat\\.tsv)\",input_file)\n",
    "    return matched.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "\n",
    "def test_get_original_file_from_aq_reformat():\n",
    "    assert get_original_file_from_aq_reformat(\"yeast_report_fastafiltered.tsv.some.oth.erstuff.spectronaut_fragion_isotopes.aq_reformat.tsv\") == \"yeast_report_fastafiltered.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def import_config_dict():\n",
    "    config_dict = load_config(INTABLE_CONFIG)\n",
    "    return config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_samplemap(samplemap_file):\n",
    "    file_ext = os.path.splitext(samplemap_file)[-1]\n",
    "    if file_ext=='.csv':\n",
    "        sep=','\n",
    "    if (file_ext=='.tsv') | (file_ext=='.txt'):\n",
    "        sep='\\t'\n",
    "\n",
    "    if 'sep' not in locals():\n",
    "        print(f\"neither of the file extensions (.tsv, .csv, .txt) detected for file {samplemap_file}! Trying with tab separation. In the case that it fails, please add the appropriate extension to your file name.\")\n",
    "        sep = \"\\t\"\n",
    "\n",
    "    return pd.read_csv(samplemap_file, sep = sep, encoding ='latin1', dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_loaded_tables(data_df, samplemap_df):\n",
    "    \"\"\"\n",
    "    Integrates information from the peptide/ion data and the samplemap, selects the relevant columns and log2 transforms intensities.\n",
    "    \"\"\"\n",
    "    samplemap_df = samplemap_df[samplemap_df[\"condition\"]!=\"\"] #remove rows that have no condition entry\n",
    "    filtvec_not_in_data = [(x in data_df.columns) for x in samplemap_df[\"sample\"]] #remove samples that are not in the dataframe\n",
    "    samplemap_df = samplemap_df[filtvec_not_in_data]\n",
    "    headers = ['protein'] + samplemap_df[\"sample\"].to_list()\n",
    "    data_df = data_df.set_index(\"ion\")\n",
    "    for sample in samplemap_df[\"sample\"]:\n",
    "        data_df[sample] = np.log2(data_df[sample].replace(0, np.nan))\n",
    "    return data_df[headers], samplemap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "#| export\n",
    "class LongTableReformater():\n",
    "    \"\"\"Generic class to reformat tabular files in chunks. For the specific cases you can inherit the class and specify reformat and iterate function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_file):\n",
    "        self._input_file = input_file\n",
    "        self._reformatting_function = None\n",
    "        self._iterator_function = self.__initialize_df_iterator__\n",
    "        self._concat_list = []\n",
    "\n",
    "    def reformat_and_load_acquisition_data_frame(self):\n",
    "\n",
    "        input_df_it = self._iterator_function()\n",
    "        \n",
    "        input_df_list = []\n",
    "        for input_df_subset in input_df_it:\n",
    "            input_df_subset = self._reformatting_function(input_df_subset)\n",
    "            input_df_list.append(input_df_subset)\n",
    "        input_df = pd.concat(input_df_list)\n",
    "        \n",
    "        return input_df\n",
    "\n",
    "    def reformat_and_save_acquisition_data_frame(self, output_file):\n",
    "        \n",
    "        input_df_it = self._iterator_function()\n",
    "        write_header = True\n",
    "        \n",
    "        for input_df_subset in input_df_it:\n",
    "            input_df_subset = self._reformatting_function(input_df_subset)\n",
    "            self.__write_reformatted_df_to_file__(input_df_subset, output_file, write_header)\n",
    "            write_header = False\n",
    "\n",
    "    def __initialize_df_iterator__(self):\n",
    "        return pd.read_csv(self._input_file, sep = \"\\t\", encoding ='latin1', chunksize=1000000)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __write_reformatted_df_to_file__(reformatted_df, filepath ,write_header):\n",
    "        reformatted_df.to_csv(filepath, header=write_header, mode='a', sep = \"\\t\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "class AcquisitionTableHandler():\n",
    "    def __init__(self, results_dir, samples):\n",
    "        self._table_infos = AcquisitionTableInfo(results_dir=results_dir)\n",
    "        self._header_infos = AcquisitionTableHeaders(self._table_infos)\n",
    "        self._samples = self.__reformat_samples_if_necessary(samples)\n",
    "    \n",
    "    def get_acquisition_info_df(self):\n",
    "        return self.__get_reformated_df__()\n",
    "\n",
    "    def save_dataframe_as_new_acquisition_dataframe(self):\n",
    "        self._output_paths = AcquisitionTableOutputPaths(self._table_infos)\n",
    "        self.__remove_possible_pre_existing_ml_table__(self._output_paths.output_file_name)\n",
    "        df_reformater = AcquisitionTableReformater(table_infos = self._table_infos, header_infos=self._header_infos, samples = self._samples, dataframe_already_preformated=False)\n",
    "        df_reformater.reformat_and_save_acquisition_data_frame(self._output_paths.output_file_name)\n",
    "\n",
    "    def update_ml_file_location_in_method_parameters_yaml(self):\n",
    "        method_params = load_method_parameters(self._table_infos._results_dir)\n",
    "        if self._output_paths == None:\n",
    "            raise Exception(\"output paths not initialized! This could be because no dataframe was saved before\")\n",
    "        method_params[self._output_paths.ml_file_accession_in_yaml] = self._output_paths.output_file_name\n",
    "        save_dict_as_yaml(method_params, self._output_paths.method_parameters_yaml_path)\n",
    "    \n",
    "    def __get_reformated_df__(self):\n",
    "        df_reformater = AcquisitionTableReformater(table_infos = self._table_infos, header_infos=self._header_infos, samples = self._samples, dataframe_already_preformated=True)\n",
    "        df = df_reformater.reformat_and_load_acquisition_data_frame()\n",
    "        return df.convert_dtypes()\n",
    "\n",
    "    def __reformat_samples_if_necessary(self, samples):\n",
    "        if \"plexDIA\" in  self._table_infos._input_type:\n",
    "            return self.__get_plexDIA_samplenames__(samples)\n",
    "        else:\n",
    "            return samples\n",
    "    \n",
    "    def __get_plexDIA_samplenames__(self, samples):\n",
    "        new_samples = []\n",
    "        for sample in samples:\n",
    "            new_samples.append(self.__get_samplename_without_mtraq_tag__(sample))\n",
    "        return new_samples\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_samplename_without_mtraq_tag__(samplename):\n",
    "        pattern = \"(.*)(_\\(mTRAQ-n-.\\))\"\n",
    "        matched = re.match(pattern, samplename)\n",
    "        return matched.group(1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __remove_possible_pre_existing_ml_table__(output_file_name):\n",
    "        if os.path.exists(output_file_name):\n",
    "            os.remove(output_file_name)\n",
    "            print(f\"removed pre existing {output_file_name}\")\n",
    "\n",
    "\n",
    "class AcquisitionTableInfo():\n",
    "    def __init__(self, results_dir, sep = \"\\t\", decimal = \".\"):\n",
    "        self._results_dir = results_dir\n",
    "        self._sep = sep\n",
    "        self._decimal = decimal\n",
    "        self._method_params_dict = load_method_parameters(results_dir)\n",
    "        self._input_file = self.__get_input_file__()\n",
    "        self._file_ending_of_formatted_table = \".ml_info_table.tsv\"\n",
    "        self.already_formatted =  self.__check_if_input_file_is_already_formatted__()\n",
    "        self._input_type, self._config_dict = self.__get_input_type_and_config_dict__()\n",
    "        self._sample_column = self.__get_sample_column__()\n",
    "        self.last_ion_level_to_use = self.__get_last_ion_level_to_use__()\n",
    "\n",
    "    def __get_input_file__(self):\n",
    "        if self._method_params_dict.get('ml_input_file') is None:\n",
    "            return self.__get_location_of_original_file__()\n",
    "        else:\n",
    "            return self._method_params_dict.get('ml_input_file')\n",
    "\n",
    "    def __check_if_input_file_is_already_formatted__(self):\n",
    "        if self._file_ending_of_formatted_table in self._input_file:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __get_input_type_and_config_dict__(self):\n",
    "        if self.already_formatted:\n",
    "            original_file = self.__get_location_of_original_file__()\n",
    "        else:\n",
    "            original_file = self._input_file\n",
    "        input_type, config_dict, _ = get_input_type_and_config_dict(original_file)\n",
    "        return input_type, config_dict\n",
    "    \n",
    "    def __get_location_of_original_file__(self):\n",
    "        input_file = self._method_params_dict.get('input_file')\n",
    "        return self.__get_original_filename_from_input_file__(input_file)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_original_filename_from_input_file__(input_file):\n",
    "        pattern = \"(.*\\.tsv|.*\\.csv|.*\\.txt)(\\..*)(.aq_reformat.tsv)\"\n",
    "        m = re.match(pattern=pattern, string=input_file)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "        else:\n",
    "            return input_file\n",
    "\n",
    "    \n",
    "    def __get_sample_column__(self):\n",
    "        return self._config_dict.get(\"sample_ID\")\n",
    "        \n",
    "    def __get_last_ion_level_to_use__(self):\n",
    "        return self._config_dict[\"ml_level\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AcquisitionTableHeaders():\n",
    "    def __init__(self, acquisition_table_info):\n",
    "\n",
    "        self._table_info = acquisition_table_info\n",
    "\n",
    "        self._ion_hierarchy = self.__get_ordered_ion_hierarchy__()\n",
    "        self._included_levelnames = self.__get_included_levelnames__()\n",
    "        self._ion_headers_grouped = self.__get_ion_headers_grouped__()\n",
    "        self._ion_headers = self.__get_ion_headers__()\n",
    "        self._numeric_headers = self.__get_numeric_headers__()\n",
    "        self._relevant_headers = self.__get_relevant_headers__()\n",
    "    \n",
    "    def __get_ordered_ion_hierarchy__(self):\n",
    "        ion_hierarchy = self._table_info._config_dict.get(\"ion_hierarchy\")\n",
    "        hier_key = 'fragion' if 'fragion' in ion_hierarchy.keys() else list(ion_hierarchy.keys())[0]\n",
    "        ion_hierarchy_on_chosen_key = ion_hierarchy.get(hier_key)\n",
    "        return ion_hierarchy_on_chosen_key\n",
    "\n",
    "    def __get_included_levelnames__(self):\n",
    "        levelnames = self.__get_all_levelnames__(self._ion_hierarchy)\n",
    "        last_ionlevel_idx = levelnames.index(self._table_info.last_ion_level_to_use)\n",
    "        return levelnames[:last_ionlevel_idx+1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_all_levelnames__(ion_hierarchy):\n",
    "        return  ion_hierarchy.get('order')\n",
    "\n",
    "    def __get_ion_headers_grouped__(self):\n",
    "        mapping_dict = self.__get_levelname_mapping_dict(self._ion_hierarchy)\n",
    "        return [mapping_dict.get(x) for x in self._included_levelnames]#on each level there can be multiple names, so it is a list of lists\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_levelname_mapping_dict(ion_hierarchy):\n",
    "        return ion_hierarchy.get('mapping')\n",
    "    \n",
    "    def __get_ion_headers__(self):\n",
    "        return list(itertools.chain(*self._ion_headers_grouped))\n",
    "\n",
    "    \n",
    "    def __get_relevant_headers__(self):\n",
    "        relevant_headers = self._numeric_headers+self._ion_headers + [self._table_info._sample_column]\n",
    "        return self.__remove_possible_none_values_from_list__(relevant_headers)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __remove_possible_none_values_from_list__(list):\n",
    "        return [x for x in list if x is not None]\n",
    "\n",
    "    def __get_numeric_headers__(self):\n",
    "        df_sample = pd.read_csv(self._table_info._input_file, sep = self._table_info._sep, decimal = self._table_info._decimal, encoding='latin1', nrows=3000) #sample 3000 rows from the df to assess the types of each row\n",
    "        df_sample = df_sample.replace({False: 0, True: 1})\n",
    "        numeric_headers =  list(df_sample.select_dtypes(include=np.number).columns)\n",
    "        numeric_headers = AcquisitionTableHeaderFilter().filter_numeric_headers_if_specified(input_type = self._table_info._input_type, numeric_headers = numeric_headers)\n",
    "        return numeric_headers\n",
    "\n",
    "\n",
    "class AcquisitionTableOutputPaths():\n",
    "    def __init__(self, table_info):\n",
    "        self._table_info = table_info\n",
    "        self.output_file_name = self.__get_output_file_name__()\n",
    "        self.method_parameters_yaml_path = self.__get_method_parameters_yaml_path__()\n",
    "        self.ml_file_accession_in_yaml = \"ml_input_file\"\n",
    "\n",
    "    def __get_output_file_name__(self):\n",
    "        old_file_name = self._table_info._input_file\n",
    "        new_file_name = old_file_name+self._table_info._file_ending_of_formatted_table\n",
    "        return new_file_name\n",
    "\n",
    "    def __get_method_parameters_yaml_path__(self):\n",
    "        return f\"{self._table_info._results_dir}/aq_parameters.yaml\"\n",
    "\n",
    "\n",
    "class AcquisitionTableReformater(LongTableReformater):\n",
    "    def __init__(self, table_infos, header_infos, samples, dataframe_already_preformated = False):\n",
    "        \n",
    "        LongTableReformater.__init__(self, table_infos._input_file)\n",
    "        self._table_infos = table_infos\n",
    "        self._header_infos = header_infos\n",
    "        self._samples = samples\n",
    "        self._dataframe_already_preformated = dataframe_already_preformated\n",
    "\n",
    "        #set the two functions that specify the explicit reformatting\n",
    "        self._reformatting_function = self.__reformatting_function__\n",
    "        self._iterator_function = self.__initialize_iterator_with_specified_columns__\n",
    "    \n",
    "    def __reformatting_function__(self, input_df_subset):\n",
    "        input_df_subset = input_df_subset.drop_duplicates()\n",
    "        input_df_subset = self.__filter_reformated_df_if_necessary__(input_df_subset)\n",
    "        if not self._dataframe_already_preformated:\n",
    "            input_df_subset = add_merged_ionnames(input_df_subset, self._header_infos._included_levelnames, self._header_infos._ion_headers_grouped, None, None)\n",
    "        return input_df_subset\n",
    "\n",
    "    def __filter_reformated_df_if_necessary__(self, reformatted_df):\n",
    "        if 'spectronaut' in self._table_infos._input_type or 'diann' in self._table_infos._input_type:\n",
    "            return self.__filter_reformatted_dataframe_to_relevant_samples__(reformatted_df)\n",
    "        else:\n",
    "            return reformatted_df\n",
    "\n",
    "    def __filter_reformatted_dataframe_to_relevant_samples__(self, input_df_subset):\n",
    "        return input_df_subset[[x in self._samples for x in input_df_subset[self._table_infos._sample_column]]]\n",
    "    \n",
    "    def __initialize_iterator_with_specified_columns__(self):\n",
    "        cols_to_use = self.__get_cols_to_use__()\n",
    "        return pd.read_csv(self._table_infos._input_file, sep = self._table_infos._sep, decimal=self._table_infos._decimal, usecols = cols_to_use, encoding ='latin1', chunksize=1000000)\n",
    "\n",
    "    def __get_cols_to_use__(self):\n",
    "        cols_to_use = self._header_infos._relevant_headers\n",
    "        if self._dataframe_already_preformated:\n",
    "            return cols_to_use+['ion']\n",
    "        else:\n",
    "            return cols_to_use\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AcquisitionTableHeaderFilter():\n",
    "    def __init__(self):\n",
    "        self._spectronaut_header_filter = lambda x : ((\"EG.\" in x) | (\"FG.\" in x)) and (\"Global\" not in x)\n",
    "        self._maxquant_header_filter = lambda x : (\"Intensity\" not in x) and (\"Experiment\" not in x)\n",
    "\n",
    "    def filter_numeric_headers_if_specified(self, input_type, numeric_headers):\n",
    "        if 'spectronaut' in input_type:\n",
    "            return [x for x in numeric_headers if self._spectronaut_header_filter(x)]\n",
    "        elif 'maxquant' in input_type:\n",
    "            return [x for x in numeric_headers if self._maxquant_header_filter(x)]\n",
    "        else:\n",
    "            return numeric_headers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def merge_acquisition_df_parameter_df(acquisition_df, parameter_df, groupby_merge_type = 'mean'):\n",
    "    \"\"\"acquisition df contains details on the acquisition, parameter df are the parameters derived from the tree\n",
    "    \"\"\"\n",
    "    merged_df = parameter_df.merge(acquisition_df, how = 'left', on = 'ion')\n",
    "    if groupby_merge_type == 'mean':\n",
    "        merged_df = merged_df.groupby('ion').mean().reset_index()\n",
    "    if groupby_merge_type == 'min':\n",
    "        merged_df = merged_df.groupby('ion').min().reset_index()\n",
    "    if groupby_merge_type == 'max':\n",
    "        merged_df = merged_df.groupby('ion').max().reset_index()\n",
    "    merged_df = merged_df.dropna(axis=1, how='all')\n",
    "    return merged_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test input parsing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the in-memory and out-of-memory longformat table loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/constantin/workspace/alphabase/nbdev_nbs/quant_reader\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from alphabase import quant_reader\n",
    "\n",
    "print(os.path.abspath(\".\"))\n",
    "input_file = \"../../test_data/unit_tests/input_table_formats/spectronaut.frgions.large.tsv\"\n",
    "outdir = \"../../test_data/unit_tests/input_table_formats/loading_comparisons\"\n",
    "\n",
    "file_default = \"default_out.tsv\"\n",
    "file_dask_proc = \"dask_proc_out.tsv\"\n",
    "\n",
    "\n",
    "def test_table_loadings(input_file, outdir, file_default, file_dask_proc):\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "        #os.chdir(outdir)\n",
    "\n",
    "    input_type, config_dict_for_type, sep = quant_reader.get_input_type_and_config_dict(input_file)\n",
    "\n",
    "    quant_reader.reformat_and_write_longtable_according_to_config(input_file,outfile_name=file_dask_proc,config_dict_for_type=config_dict_for_type, enforce_largefile_processing=True, chunksize=10_000)\n",
    "    quant_reader.reformat_and_write_longtable_according_to_config(input_file,outfile_name=file_default,config_dict_for_type=config_dict_for_type, chunksize=10_000)\n",
    "\n",
    "    df_default = pd.read_csv(file_default, sep = \"\\t\")\n",
    "\n",
    "    df_dask_proc = pd.read_csv(file_dask_proc, sep = \"\\t\")\n",
    "\n",
    "\n",
    "    assert df_default.equals(df_dask_proc)\n",
    "\n",
    "    os.remove(file_default)\n",
    "    os.remove(file_dask_proc)\n",
    "    shutil.rmtree(outdir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_table_loadings(input_file, outdir, file_default, file_dask_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using input type diann_fragion_isotopes\n",
      "loading ran through\n",
      "using input type spectronaut_precursor_v2\n",
      "loading ran through\n",
      "using input type spectronaut_fragion_isotopes\n",
      "loading ran through\n",
      "using input type maxquant_peptides_leading_razor_protein\n",
      "loading ran through\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "\n",
    "import alphabase.quant_reader as quant_reader\n",
    "import os\n",
    "import shutil\n",
    "outdir = \"../../test_data/unit_tests/input_table_formats/\"\n",
    "\n",
    "tabledir =\"../../test_data/unit_tests/input_table_formats/\"\n",
    "results_dir = \"../../test_data/unit_tests/results/\"\n",
    "\n",
    "\n",
    "input_files = [os.path.join(tabledir, x ) for x in [\"diann.tsv\", \"spectronaut.tsv\", \"spectronaut_frgion.tsv\", \"mq_peptides.txt\"]]\n",
    "samplemap_files = [os.path.join(tabledir, x ) for x in [\"samplemap.diann.tsv\", \"samplemap.spectronaut.tsv\", \"samplemap.spectronaut.frgions.tsv\", \"samplemap.mq.tsv\"]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def perform_table_loading(input_file, samplemap_file):\n",
    "    \"\"\"only makes sure that the commands run without error\"\"\"\n",
    "    \n",
    "    #import the input table once the input and the results folder are specified. \n",
    "    # The function automatically recognizes the format (Currently MQ, Spectronaut, DIA-NN configured)\n",
    "    input_data = quant_reader.import_data(input_file)\n",
    "\n",
    "for idx in range(len(input_files)):\n",
    "    perform_table_loading(input_files[idx], samplemap_files[idx])\n",
    "    print('loading ran through')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def compare_generic_table_with_original(preprocessed_input_df, original_input_file, config_yaml,input_typename_config, sep = \"\\t\"):\n",
    "    id2quant_orig, id2quant_preproc = get_processed_original_id2quant_maps(preprocessed_input_df, original_input_file, config_yaml,input_typename_config)\n",
    "    keys_orig = set(id2quant_orig.keys())\n",
    "    keys_preproc = set(id2quant_preproc.keys())\n",
    "    keydiff = keys_preproc.difference(keys_orig)\n",
    "    keys_orig = sorted(keys_orig)\n",
    "    keys_preproc = sorted(keys_preproc)\n",
    "    print(list(keys_orig)[:10])\n",
    "    print(list(keys_preproc)[:10])\n",
    "    \n",
    "    \n",
    "    assert(len(keydiff)==0) #check that all keys in the preprocessed set are part of the original set\n",
    "\n",
    " #   venn2([set(id2quant_orig.keys()), set(id2quant_preproc.keys())], [\"original\", \"preprocessed\"])\n",
    "    \n",
    "    quantvec_orig = np.array([id2quant_orig.get(x)for x in id2quant_preproc.keys()])\n",
    "    quantvec_preproc = np.array([id2quant_preproc.get(x)for x in id2quant_preproc.keys()])\n",
    "    unequal_quant = [id2quant_orig.get(x)!=id2quant_preproc.get(x) for x in id2quant_preproc.keys()]\n",
    "    unequal_quant_scaled = [id2quant_orig.get(x)*10000!=id2quant_preproc.get(x) for x in id2quant_preproc.keys()]\n",
    "    print(sum(unequal_quant))\n",
    "    print(sum(unequal_quant_scaled))\n",
    "    plt.show()\n",
    "    plt.scatter(quantvec_orig, quantvec_preproc)\n",
    "    plt.show()\n",
    "    corrcoeff = np.corrcoef(quantvec_orig,quantvec_preproc)[0][1]\n",
    "    print(f\"correlation between both processings: {corrcoeff}\")\n",
    "    assert(corrcoeff>0.999)\n",
    "\n",
    "\n",
    "\n",
    "def get_processed_original_id2quant_maps(preprocessed_input_df, original_input_file, config_yaml,input_typename_config, sep = \"\\t\"):\n",
    "    config_all = yaml.safe_load(open(config_yaml, 'r'))\n",
    "    config_dict = config_all.get(input_typename_config)\n",
    "    id_cols = config_dict.get(\"ion_cols\") + [config_dict.get(\"sample_ID\")]\n",
    "    quant_col = list(config_dict.get(\"quant_ID\").values())\n",
    "    id2quant_orig = get_id2quant_original(original_input_file, id_cols, quant_col, sep)\n",
    "    id2quant_preproc = get_id2quant_processed(preprocessed_input_df, id_cols, quant_col)\n",
    "    return id2quant_orig, id2quant_preproc\n",
    "\n",
    "\n",
    "def get_id2quant_original(original_input_file, id_cols, quant_col, sep):\n",
    "    print(id_cols)\n",
    "    print(id_cols+quant_col)\n",
    "    orig_df = pd.read_csv(original_input_file, sep=sep, usecols= id_cols+quant_col)\n",
    "    orig_df[\"compareID\"] = orig_df[id_cols].astype('string').sum(axis = 1)\n",
    "    display(orig_df)\n",
    "    id2quant = dict(zip(orig_df[\"compareID\"], orig_df[quant_col[0]]))\n",
    "    id2quant = {k: round(v,3) for k, v in id2quant.items()}\n",
    "    return id2quant\n",
    "\n",
    "\n",
    "def get_id2quant_processed(preprocessed_input_df, id_cols, quant_col):\n",
    "    compare_IDs = []\n",
    "    quantvals = []\n",
    "    for column in preprocessed_input_df.columns:\n",
    "        if(column == \"protein\"):\n",
    "            continue\n",
    "        id = pd.Series([column for x in range(len(preprocessed_input_df.index))]).to_numpy()[0]\n",
    "        reformated_pep_id = [x.split(\"_MOD_\")[1].replace(\"_CHARGE_\", \"\")[:-1]+id for x in preprocessed_input_df.index]\n",
    "        compare_IDs.extend(reformated_pep_id)\n",
    "\n",
    "        quantvals.extend(list(preprocessed_input_df[column]))\n",
    "    \n",
    "    id2quant = dict(zip(compare_IDs, quantvals))\n",
    "    id2quant = {k: round(2**v,3) for k, v in id2quant.items() if ~np.isnan(v)}\n",
    "    return id2quant\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using input type spectronaut_precursor_v2\n",
      "['EG.ModifiedPeptide', 'FG.Charge', 'R.Label']\n",
      "['EG.ModifiedPeptide', 'FG.Charge', 'R.Label', 'FG.Quantity']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R.Label</th>\n",
       "      <th>EG.ModifiedPeptide</th>\n",
       "      <th>FG.Charge</th>\n",
       "      <th>FG.Quantity</th>\n",
       "      <th>compareID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_LNVLPVDVLTR_</td>\n",
       "      <td>2</td>\n",
       "      <td>95427.601562</td>\n",
       "      <td>_LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_LNVLPVDVLTR_</td>\n",
       "      <td>2</td>\n",
       "      <td>95427.601562</td>\n",
       "      <td>_LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_LNVLPVDVLTR_</td>\n",
       "      <td>2</td>\n",
       "      <td>95427.601562</td>\n",
       "      <td>_LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_LNVLPVDVLTR_</td>\n",
       "      <td>2</td>\n",
       "      <td>95427.601562</td>\n",
       "      <td>_LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_LNVLPVDVLTR_</td>\n",
       "      <td>2</td>\n",
       "      <td>95427.601562</td>\n",
       "      <td>_LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_</td>\n",
       "      <td>2</td>\n",
       "      <td>110807.851562</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_</td>\n",
       "      <td>2</td>\n",
       "      <td>110807.851562</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_</td>\n",
       "      <td>2</td>\n",
       "      <td>110807.851562</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_</td>\n",
       "      <td>2</td>\n",
       "      <td>110807.851562</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_</td>\n",
       "      <td>2</td>\n",
       "      <td>110807.851562</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          R.Label  \\\n",
       "0     E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "1     E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "2     E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "3     E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "4     E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "...                                           ...   \n",
       "9994  E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "9995  E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "9996  E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "9997  E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "9998  E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "\n",
       "                    EG.ModifiedPeptide  FG.Charge    FG.Quantity  \\\n",
       "0                        _LNVLPVDVLTR_          2   95427.601562   \n",
       "1                        _LNVLPVDVLTR_          2   95427.601562   \n",
       "2                        _LNVLPVDVLTR_          2   95427.601562   \n",
       "3                        _LNVLPVDVLTR_          2   95427.601562   \n",
       "4                        _LNVLPVDVLTR_          2   95427.601562   \n",
       "...                                ...        ...            ...   \n",
       "9994  _GC[Carbamidomethyl (C)]VITISGR_          2  110807.851562   \n",
       "9995  _GC[Carbamidomethyl (C)]VITISGR_          2  110807.851562   \n",
       "9996  _GC[Carbamidomethyl (C)]VITISGR_          2  110807.851562   \n",
       "9997  _GC[Carbamidomethyl (C)]VITISGR_          2  110807.851562   \n",
       "9998  _GC[Carbamidomethyl (C)]VITISGR_          2  110807.851562   \n",
       "\n",
       "                                              compareID  \n",
       "0     _LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...  \n",
       "1     _LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...  \n",
       "2     _LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...  \n",
       "3     _LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...  \n",
       "4     _LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...  \n",
       "...                                                 ...  \n",
       "9994  _GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...  \n",
       "9995  _GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...  \n",
       "9996  _GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...  \n",
       "9997  _GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...  \n",
       "9998  _GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...  \n",
       "\n",
       "[9999 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_AAAAASAAGPGGLVAGKEEK_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAAAASAAGPGGLVAGKEEK_3E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAAAASAAGPGGLVAGK_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAANFSDR_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAAPEKPDAEHDAPQFIEPLDSIDR_4E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAFDVIVR_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAGAILK_1E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAGEHIASSGK_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAGIWYEHR_3E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AALAATGAASGGGGGGGGAGSR_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms']\n",
      "['_AAAAASAAGPGGLVAGKEEK_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAAAASAAGPGGLVAGKEEK_3E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAAAASAAGPGGLVAGK_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAANFSDR_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAAPEKPDAEHDAPQFIEPLDSIDR_4E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAFDVIVR_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAGAILK_1E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAGEHIASSGK_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAGIWYEHR_3E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AALAATGAASGGGGGGGGAGSR_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms']\n",
      "49\n",
      "1716\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation between both processings: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file = os.path.join(tabledir, \"spectronaut.tsv\")\n",
    "samplemap_file = os.path.join(tabledir, \"samplemap.spectronaut.tsv\")\n",
    "\n",
    "\n",
    "input_data = quant_reader.import_data(input_file)\n",
    "samplemap_df = quant_reader.load_samplemap(samplemap_file)\n",
    "input_processed, samplemap_df_processed = quant_reader.prepare_loaded_tables(input_data, samplemap_df)\n",
    "compare_generic_table_with_original(input_processed, input_file, quant_reader.INTABLE_CONFIG, \"spectronaut_precursor_v2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphabase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aab52e70b8233fcc2c85387ee72fd5633b026a7de98e9584d2ab8bc34679247a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
