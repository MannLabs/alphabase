{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:12:12.969079Z",
     "start_time": "2021-10-15T07:12:12.963968Z"
    }
   },
   "outputs": [],
   "source": [
    "# default_exp io.hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:13:23.383266Z",
     "start_time": "2021-10-15T07:13:23.373138Z"
    }
   },
   "source": [
    "This module provides a common interface to access HDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:41:06.225778Z",
     "start_time": "2021-10-21T07:41:05.699433Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of relying directly on the `h5py` interface, we will use an HDF wrapper file to provide consistent access to only those specific HDF features we want. Since components of an HDF file come in three shapes `datasets`, `groups` and `attributes`, we will first define a generic HDF wrapper object to handle these components. Once this is done, the HDF wrapper file can be treated as such an object with additional features to open and close the initial connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:41:06.361696Z",
     "start_time": "2021-10-21T07:41:06.326484Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class HDF_Object_Wrapper(object):\n",
    "    '''\n",
    "    A generic class to access HDF components.\n",
    "    '''\n",
    "\n",
    "    @property\n",
    "    def read_only(self):\n",
    "        return self._read_only\n",
    "    \n",
    "    @property\n",
    "    def truncate(self):\n",
    "        return self._truncate\n",
    "    \n",
    "    @property\n",
    "    def hdf_parent_file_name(self):\n",
    "        return self._hdf_parent_file_name\n",
    "    \n",
    "    @property\n",
    "    def hdf_parent_group_name(self):\n",
    "        return self._hdf_parent_group_name\n",
    "        \n",
    "    @property\n",
    "    def values(self):\n",
    "        return self[...]\n",
    "          \n",
    "    @property\n",
    "    def metadata(self):\n",
    "        with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "            return dict(hdf_file[self.hdf_parent_group_name].attrs)\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self._dtype\n",
    "        \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._shape\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        hdf_parent_file_name: str,\n",
    "        hdf_parent_group_name: str,\n",
    "        read_only: bool = True,\n",
    "        truncate: bool = False,\n",
    "    ):\n",
    "        object.__setattr__(self, \"_read_only\", read_only)\n",
    "        object.__setattr__(self, \"_truncate\", truncate)\n",
    "        object.__setattr__(\n",
    "            self,\n",
    "            \"_hdf_parent_file_name\",\n",
    "            hdf_parent_file_name\n",
    "        )\n",
    "        object.__setattr__(\n",
    "            self,\n",
    "            \"_hdf_parent_group_name\",\n",
    "            hdf_parent_group_name\n",
    "        )\n",
    "        with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "            hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "            for name, value in hdf_object.attrs.items():\n",
    "                object.__setattr__(self, name, value)\n",
    "            if isinstance(hdf_object, h5py.Dataset):\n",
    "                object.__setattr__(self, \"_dtype\", hdf_object.dtype)\n",
    "                object.__setattr__(self, \"_shape\", hdf_object.shape)\n",
    "            else:              \n",
    "                for name in hdf_object:\n",
    "                    subobject = HDF_Object_Wrapper(\n",
    "                        hdf_parent_file_name=self.hdf_parent_file_name,\n",
    "                        hdf_parent_group_name=f\"{self.hdf_parent_group_name}/{name}\",\n",
    "                        read_only=self.read_only,\n",
    "                        truncate=self.truncate,\n",
    "                    )\n",
    "                    object.__setattr__(self, name, subobject)\n",
    "                if \"is_pd_dataframe\" in hdf_object.attrs:\n",
    "                    object.__setattr__(self, \"_dtype\", \"dataframe\")\n",
    "                    object.__setattr__(\n",
    "                        self,\n",
    "                        \"_shape\",\n",
    "                        (\n",
    "                            subobject.shape,\n",
    "                            len(hdf_object)\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    object.__setattr__(self, \"_dtype\", \"group\")\n",
    "                    object.__setattr__(self, \"_shape\", len(hdf_object))\n",
    "\n",
    "    def __iter__(self):\n",
    "        with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "            hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "            if isinstance(self.dtype, str):\n",
    "                for name in hdf_object:    \n",
    "                    yield self.__getattribute__(name)\n",
    "            else:\n",
    "                for i in hdf_object:    \n",
    "                    yield i\n",
    "                    \n",
    "    def __eq__(self, other):\n",
    "        return (\n",
    "            self.hdf_parent_file_name == other.self.hdf_parent_file_name\n",
    "        ) and (\n",
    "            self.hdf_parent_group_name == other.hdf_parent_group_name\n",
    "        )\n",
    "                    \n",
    "    def set_read_only(self, read_only: bool = True):\n",
    "        object.__setattr__(self, \"_read_only\", read_only)\n",
    "        if isinstance(self.dtype, str):\n",
    "            for subset in self:\n",
    "                subset.set_read_only(read_only)\n",
    "        \n",
    "    def set_truncate(self, truncate: bool = True):\n",
    "        object.__setattr__(self, \"_truncate\", truncate)\n",
    "        if isinstance(self.dtype, str):\n",
    "            for subset in self:\n",
    "                subset.set_truncate(truncate)\n",
    "\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def modify(self, read_only=False, truncate=True):\n",
    "        original_read_only = self.read_only\n",
    "        original_truncate = self.truncate\n",
    "        try:\n",
    "            self.set_read_only(read_only)\n",
    "            self.set_truncate(truncate)\n",
    "            yield self\n",
    "        finally:\n",
    "            self.set_read_only(original_read_only)\n",
    "            self.set_truncate(original_truncate)\n",
    "                \n",
    "    def __setattr__(self, name, value):\n",
    "        if self.read_only:\n",
    "            raise AttributeError(\"Cannot set read-only attributes\")\n",
    "        elif not isinstance(name, str):\n",
    "            raise KeyError(f\"Attribute name '{name}' is not a string\")\n",
    "        elif not bool(re.match(r'^[a-zA-Z_][\\w.-]*$', name)):\n",
    "            raise KeyError(f\"Invalid attribute name: {name}\")\n",
    "        with h5py.File(self.hdf_parent_file_name, \"a\") as hdf_file:\n",
    "            hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "            exists = (name in hdf_object) or (name in hdf_object.attrs)\n",
    "            if exists:\n",
    "                if not self.truncate:\n",
    "                    raise KeyError(\n",
    "                        f\"Attribute name '{name}' cannot be truncated\"\n",
    "                    )\n",
    "            if isinstance(value, (str, bool, int, float)):\n",
    "                hdf_object.attrs[name] = value\n",
    "                parsed_value = value\n",
    "            elif isinstance(value, (np.ndarray, pd.core.series.Series)):\n",
    "                parsed_value = self.__create_new_dataset(\n",
    "                    name,\n",
    "                    value,\n",
    "                    hdf_object,\n",
    "                    exists,\n",
    "                )\n",
    "            elif isinstance(value, (dict, pd.DataFrame)):\n",
    "                parsed_value = self.__create_new_group(\n",
    "                    name,\n",
    "                    value,\n",
    "                    hdf_object,\n",
    "                    exists,\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Invalid attribute value {value}\")\n",
    "            object.__setattr__(self, name, parsed_value)\n",
    "    \n",
    "    def __create_new_dataset(\n",
    "        self,\n",
    "        name:str,\n",
    "        array: np.ndarray,\n",
    "        hdf_object: h5py.Group,\n",
    "        exists: bool,\n",
    "    ):\n",
    "        if exists:\n",
    "            del hdf_object[name]\n",
    "        if isinstance(array, (pd.core.series.Series)):\n",
    "            array = array.values\n",
    "        hdf_dataset = hdf_object.create_dataset(\n",
    "            name,\n",
    "            data=array,\n",
    "#             TODO\n",
    "            compression=\"lzf\",\n",
    "#             # compression=\"gzip\" if compress else None, # TODO slower to make, faster to load?\n",
    "            shuffle=True,\n",
    "            chunks=True,\n",
    "            maxshape=tuple([None for i in array.shape]),\n",
    "        )\n",
    "        new_array = HDF_Object_Wrapper(\n",
    "            hdf_parent_file_name=self.hdf_parent_file_name,\n",
    "            hdf_parent_group_name=f\"{self.hdf_parent_group_name}/{name}\",\n",
    "            read_only=self.read_only,\n",
    "            truncate=self.truncate,\n",
    "        )\n",
    "        object.__setattr__(self, \"_shape\", self._shape + 1)\n",
    "        return new_array\n",
    "    \n",
    "    def __create_new_group(\n",
    "        self,\n",
    "        name:str,\n",
    "        new_dict: dict,\n",
    "        hdf_object: h5py.Group,\n",
    "        exists: bool,\n",
    "    ):\n",
    "        if exists:\n",
    "            del hdf_object[name]\n",
    "        new_group = hdf_object.create_group(name)\n",
    "        if isinstance(new_dict, pd.DataFrame):\n",
    "            new_dict = dict(new_dict)\n",
    "            new_dict[\"is_pd_dataframe\"] = True\n",
    "            is_pd_dataframe = True\n",
    "        else:\n",
    "            is_pd_dataframe = False\n",
    "        new_group = HDF_Object_Wrapper(\n",
    "            hdf_parent_file_name=self.hdf_parent_file_name,\n",
    "            hdf_parent_group_name=f\"{self.hdf_parent_group_name}/{name}\",\n",
    "            read_only=self.read_only,\n",
    "            truncate=self.truncate,\n",
    "        )\n",
    "        for key, value in new_dict.items():\n",
    "            new_group.__setattr__(key, value)\n",
    "        if is_pd_dataframe:\n",
    "            object.__setattr__(new_group, \"_dtype\", \"dataframe\")\n",
    "        object.__setattr__(self, \"_shape\", self._shape + 1)\n",
    "        return new_group\n",
    "    \n",
    "    def append(self, data):\n",
    "        if isinstance(self.dtype, str):\n",
    "            if self.dtype == \"dataframe\":\n",
    "                for column_name in self:\n",
    "                    array.append(data[column_name])\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "        else:\n",
    "            with h5py.File(self.hdf_parent_file_name, \"a\") as hdf_file:\n",
    "                hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "                new_shape = tuple([i + j for i, j in zip(self.shape, data.shape)])\n",
    "                hdf_object.resize(new_shape)\n",
    "                hdf_object[self.shape[0]: self.shape[0] + data.shape[0]] = data\n",
    "                object.__setattr__(self, \"_shape\", new_shape)\n",
    "    \n",
    "    def __getitem__(self, keys):\n",
    "        if not isinstance(self.dtype, str):\n",
    "            with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "                return hdf_file[self.hdf_parent_group_name][keys]\n",
    "        elif self.dtype == \"dataframe\":\n",
    "            with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "                hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "                arrays = {}\n",
    "                for name in hdf_object:\n",
    "                    if isinstance(hdf_object[name], h5py.Dataset):\n",
    "                        arrays[name] = hdf_object[name]\n",
    "                return pd.DataFrame(\n",
    "                    {\n",
    "                        name: array[keys] for name, array in arrays.items()\n",
    "                    }\n",
    "                )\n",
    "        elif self.dtype == \"group\":\n",
    "            with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "                hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "                if keys is Ellipsis:\n",
    "                    return {\n",
    "                        name: self.__getattribute__(name) for name in hdf_object\n",
    "                    }\n",
    "                elif keys not in hdf_object:\n",
    "                    raise KeyError(\n",
    "                        f\"No object with {keys} available in this group\"\n",
    "                    )\n",
    "                return self.__getattribute__(keys)\n",
    "            return object.self.__dict__[keys]  # TODO might be to generic?\n",
    "        else:\n",
    "            raise KeyError(\"Dtype not understood\")\n",
    "            \n",
    "            \n",
    "class HDF_File(HDF_Object_Wrapper):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hdf_parent_file_name: str,\n",
    "        *,\n",
    "        read_only: bool = True,\n",
    "        truncate: bool = False,\n",
    "        delete_existing: bool = False,\n",
    "    ):\n",
    "        if delete_existing:\n",
    "            mode = \"w\"\n",
    "        else:\n",
    "            mode = \"a\"\n",
    "        with h5py.File(hdf_parent_file_name, mode) as hdf_file:\n",
    "            pass\n",
    "        super().__init__(\n",
    "            hdf_parent_file_name=hdf_parent_file_name,\n",
    "            hdf_parent_group_name=\"/\",\n",
    "            read_only=read_only,\n",
    "            truncate=truncate,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:57:59.119927Z",
     "start_time": "2021-10-21T07:57:59.092064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   col2 col_str\n",
       " 0     0  b'str'\n",
       " 1     1    b'i'\n",
       " 2     2  b'ngs',\n",
       "    col2 col_str\n",
       " 0     0     str\n",
       " 1     1       i\n",
       " 2     2     ngs)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "\n",
    "import tempfile\n",
    "\n",
    "def test_HDF_wrappers():\n",
    "    hdf_file_name = os.path.join(tempfile.gettempdir(), \"sandbox.hdf\")\n",
    "    hdf_file = HDF_File(\n",
    "        hdf_file_name,\n",
    "        read_only=False,\n",
    "        truncate=True,\n",
    "        delete_existing=True\n",
    "    )\n",
    "    np.testing.assert_equal(0, hdf_file.shape)\n",
    "    file_size = os.path.getsize(hdf_file_name)\n",
    "    hdf_file.attr1 = 1\n",
    "    np.testing.assert_equal(1, hdf_file.attr1)\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    array = np.random.rand(10)\n",
    "    hdf_file.array = array\n",
    "    np.testing.assert_equal(array, hdf_file.array.values)\n",
    "    np.testing.assert_equal(array[:3], hdf_file.array[:3])\n",
    "    np.testing.assert_equal((10,), hdf_file.array.shape)\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    hdf_file.array.array_attr = \"some attr\"\n",
    "    np.testing.assert_equal(hdf_file.array.array_attr, \"some attr\")\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    group = {\n",
    "        \"subgroup1\": {\n",
    "            \"subsubgroup\": {},\n",
    "            \"same_array\": array,\n",
    "            \"a_bool\": True\n",
    "        },\n",
    "        \"subgroup2\": {}\n",
    "    }\n",
    "    hdf_file.group = group\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    np.testing.assert_equal(hdf_file.group.subgroup1.a_bool, True)\n",
    "    np.testing.assert_equal(hdf_file.group.subgroup1.shape, 2)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"col2\": np.arange(3),\n",
    "            \"col_str\": [\"str\", \"i\", \"ngs\"],\n",
    "        }\n",
    "    )\n",
    "    hdf_file.df = df\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    return hdf_file.df.values, df\n",
    "#     assert hdf_file.df.values.equals(df)\n",
    "    \n",
    "test_HDF_wrappers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:59:25.912271Z",
     "start_time": "2021-10-21T07:59:25.905285Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file_name = \"sandbox.hdf\"\n",
    "hdf_file = HDF_File(\n",
    "    hdf_file_name,\n",
    "    read_only=False,\n",
    "    truncate=True,\n",
    "    delete_existing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:59:26.250747Z",
     "start_time": "2021-10-21T07:59:26.244052Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file.attr1 = 1\n",
    "hdf_file.name = \"Mathhias\"\n",
    "hdf_file.group = {}\n",
    "hdf_file.group.array = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:59:26.598035Z",
     "start_time": "2021-10-21T07:59:26.594055Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file.group.array.append(np.arange(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:59:27.015129Z",
     "start_time": "2021-10-21T07:59:27.010135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf_file.group.array.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:59:27.504192Z",
     "start_time": "2021-10-21T07:59:27.494913Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file.df = pd.DataFrame(\n",
    "    {\n",
    "        \"feng\": np.arange(10),\n",
    "        \"sander\": \"my name\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T08:00:01.278658Z",
     "start_time": "2021-10-21T08:00:01.273045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_read_only': False,\n",
       " '_truncate': True,\n",
       " '_hdf_parent_file_name': 'sandbox.hdf',\n",
       " '_hdf_parent_group_name': '//df',\n",
       " '_dtype': 'dataframe',\n",
       " '_shape': 2,\n",
       " 'feng': <__main__.HDF_Object_Wrapper at 0x7fbd00ce1190>,\n",
       " 'sander': <__main__.HDF_Object_Wrapper at 0x7fbd00ce1460>}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf_file.df.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:45:46.407219Z",
     "start_time": "2021-10-21T07:45:46.403643Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"feng\": np.arange(10),\n",
    "        \"sander\": \"your name\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:47:47.607593Z",
     "start_time": "2021-10-21T07:47:47.601255Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file_name = \"sandbox.hdf\"\n",
    "hdf_file2 = HDF_File(\n",
    "    hdf_file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:48:51.369213Z",
     "start_time": "2021-10-21T07:48:51.365867Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file2.feng = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:47:59.462486Z",
     "start_time": "2021-10-21T07:47:59.458593Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file2.set_read_only(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:48:48.006689Z",
     "start_time": "2021-10-21T07:48:48.001908Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file2.set_truncate(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:50:41.939854Z",
     "start_time": "2021-10-21T07:50:41.934518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attr1': 1, 'feng': 2, 'name': 'Mathhias'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf_file2.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:54:17.826531Z",
     "start_time": "2021-10-21T07:54:10.107297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502 µs ± 3.73 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "440 µs ± 25.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hdf_file.group.array[[1,5,8]]\n",
    "%timeit hdf_file.group.array.values[[1,5,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alphabase]",
   "language": "python",
   "name": "conda-env-alphabase-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
