{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:12:12.969079Z",
     "start_time": "2021-10-15T07:12:12.963968Z"
    }
   },
   "outputs": [],
   "source": [
    "# default_exp hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:13:23.383266Z",
     "start_time": "2021-10-15T07:13:23.373138Z"
    }
   },
   "source": [
    "This module provides a common interface to access HDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:41:06.225778Z",
     "start_time": "2021-10-21T07:41:05.699433Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of relying directly on the `h5py` interface, we will use an HDF wrapper file to provide consistent access to only those specific HDF features we want. Since components of an HDF file come in three shapes `datasets`, `groups` and `attributes`, we will first define a generic HDF wrapper object to handle these components. Once this is done, the HDF wrapper file can be treated as such an object with additional features to open and close the initial connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:41:06.361696Z",
     "start_time": "2021-10-21T07:41:06.326484Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class HDF_Object_Wrapper(object):\n",
    "    '''\n",
    "    A generic class to access HDF components.\n",
    "    '''\n",
    "\n",
    "    @property\n",
    "    def read_only(self):\n",
    "        return self._read_only\n",
    "    \n",
    "    @property\n",
    "    def truncate(self):\n",
    "        return self._truncate\n",
    "    \n",
    "    @property\n",
    "    def hdf_parent_file_name(self):\n",
    "        return self._hdf_parent_file_name\n",
    "    \n",
    "    @property\n",
    "    def hdf_parent_group_name(self):\n",
    "        return self._hdf_parent_group_name\n",
    "        \n",
    "    @property\n",
    "    def values(self):\n",
    "        return self[...]\n",
    "          \n",
    "    @property\n",
    "    def metadata(self):\n",
    "        with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "            return dict(hdf_file[self.hdf_parent_group_name].attrs)\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self._dtype\n",
    "        \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._shape\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        hdf_parent_file_name: str,\n",
    "        hdf_parent_group_name: str,\n",
    "        read_only: bool = True,\n",
    "        truncate: bool = False,\n",
    "    ):\n",
    "        object.__setattr__(self, \"_read_only\", read_only)\n",
    "        object.__setattr__(self, \"_truncate\", truncate)\n",
    "        object.__setattr__(\n",
    "            self,\n",
    "            \"_hdf_parent_file_name\",\n",
    "            hdf_parent_file_name\n",
    "        )\n",
    "        object.__setattr__(\n",
    "            self,\n",
    "            \"_hdf_parent_group_name\",\n",
    "            hdf_parent_group_name\n",
    "        )\n",
    "        with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "            hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "            for name, value in hdf_object.attrs.items():\n",
    "                object.__setattr__(self, name, value)\n",
    "            if isinstance(hdf_object, h5py.Dataset):\n",
    "                object.__setattr__(self, \"_dtype\", hdf_object.dtype)\n",
    "                object.__setattr__(self, \"_shape\", hdf_object.shape)\n",
    "            else:              \n",
    "                for name in hdf_object:\n",
    "                    subobject = HDF_Object_Wrapper(\n",
    "                        hdf_parent_file_name=self.hdf_parent_file_name,\n",
    "                        hdf_parent_group_name=f\"{self.hdf_parent_group_name}/{name}\",\n",
    "                        read_only=self.read_only,\n",
    "                        truncate=self.truncate,\n",
    "                    )\n",
    "                    object.__setattr__(self, name, subobject)\n",
    "                if \"is_pd_dataframe\" in hdf_object.attrs:\n",
    "                    object.__setattr__(self, \"_dtype\", \"dataframe\")\n",
    "                    object.__setattr__(\n",
    "                        self,\n",
    "                        \"_shape\",\n",
    "                        (\n",
    "                            subobject.shape,\n",
    "                            len(hdf_object)\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    object.__setattr__(self, \"_dtype\", \"group\")\n",
    "                    object.__setattr__(self, \"_shape\", len(hdf_object))\n",
    "\n",
    "    def __iter__(self):\n",
    "        with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "            hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "            if isinstance(self.dtype, str):\n",
    "                for name in hdf_object:    \n",
    "                    yield self.__getattribute__(name)\n",
    "            else:\n",
    "                for i in hdf_object:    \n",
    "                    yield i\n",
    "                    \n",
    "    def __eq__(self, other):\n",
    "        return (\n",
    "            self.hdf_parent_file_name == other.self.hdf_parent_file_name\n",
    "        ) and (\n",
    "            self.hdf_parent_group_name == other.hdf_parent_group_name\n",
    "        )\n",
    "                    \n",
    "    def set_read_only(self, read_only: bool = True):\n",
    "        object.__setattr__(self, \"_read_only\", read_only)\n",
    "        if isinstance(self.dtype, str):\n",
    "            for subset in self:\n",
    "                subset.set_read_only(read_only)\n",
    "        \n",
    "    def set_truncate(self, truncate: bool = True):\n",
    "        object.__setattr__(self, \"_truncate\", truncate)\n",
    "        if isinstance(self.dtype, str):\n",
    "            for subset in self:\n",
    "                subset.set_truncate(truncate)\n",
    "\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def modify(self, read_only=False, truncate=True):\n",
    "        original_read_only = self.read_only\n",
    "        original_truncate = self.truncate\n",
    "        try:\n",
    "            self.set_read_only(read_only)\n",
    "            self.set_truncate(truncate)\n",
    "            yield self\n",
    "        finally:\n",
    "            self.set_read_only(original_read_only)\n",
    "            self.set_truncate(original_truncate)\n",
    "                \n",
    "    def __setattr__(self, name, value):\n",
    "        if self.read_only:\n",
    "            raise AttributeError(\"Cannot set read-only attributes\")\n",
    "        elif not isinstance(name, str):\n",
    "            raise KeyError(f\"Attribute name '{name}' is not a string\")\n",
    "        elif not bool(re.match(r'^[a-zA-Z_][\\w.-]*$', name)):\n",
    "            raise KeyError(f\"Invalid attribute name: {name}\")\n",
    "        with h5py.File(self.hdf_parent_file_name, \"a\") as hdf_file:\n",
    "            hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "            exists = (name in hdf_object) or (name in hdf_object.attrs)\n",
    "            if exists:\n",
    "                if not self.truncate:\n",
    "                    raise KeyError(\n",
    "                        f\"Attribute name '{name}' cannot be truncated\"\n",
    "                    )\n",
    "            if isinstance(value, (str, bool, int, float)):\n",
    "                hdf_object.attrs[name] = value\n",
    "                parsed_value = value\n",
    "            elif isinstance(value, (np.ndarray, pd.core.series.Series)):\n",
    "                parsed_value = self.__create_new_dataset(\n",
    "                    name,\n",
    "                    value,\n",
    "                    hdf_object,\n",
    "                    exists,\n",
    "                )\n",
    "            elif isinstance(value, (dict, pd.DataFrame)):\n",
    "                parsed_value = self.__create_new_group(\n",
    "                    name,\n",
    "                    value,\n",
    "                    hdf_object,\n",
    "                    exists,\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Invalid attribute value {value}\")\n",
    "            object.__setattr__(self, name, parsed_value)\n",
    "    \n",
    "    def __create_new_dataset(\n",
    "        self,\n",
    "        name:str,\n",
    "        array: np.ndarray,\n",
    "        hdf_object: h5py.Group,\n",
    "        exists: bool,\n",
    "    ):\n",
    "        if exists:\n",
    "            del hdf_object[name]\n",
    "        if isinstance(array, (pd.core.series.Series)):\n",
    "            array = array.values\n",
    "        hdf_dataset = hdf_object.create_dataset(\n",
    "            name,\n",
    "            data=array,\n",
    "#             TODO\n",
    "            compression=\"lzf\",\n",
    "#             # compression=\"gzip\" if compress else None, # TODO slower to make, faster to load?\n",
    "            shuffle=True,\n",
    "            chunks=True,\n",
    "            maxshape=tuple([None for i in array.shape]),\n",
    "        )\n",
    "        new_array = HDF_Object_Wrapper(\n",
    "            hdf_parent_file_name=self.hdf_parent_file_name,\n",
    "            hdf_parent_group_name=f\"{self.hdf_parent_group_name}/{name}\",\n",
    "            read_only=self.read_only,\n",
    "            truncate=self.truncate,\n",
    "        )\n",
    "        object.__setattr__(self, \"_shape\", self._shape + 1)\n",
    "        return new_array\n",
    "    \n",
    "    def __create_new_group(\n",
    "        self,\n",
    "        name:str,\n",
    "        new_dict: dict,\n",
    "        hdf_object: h5py.Group,\n",
    "        exists: bool,\n",
    "    ):\n",
    "        if exists:\n",
    "            del hdf_object[name]\n",
    "        new_group = hdf_object.create_group(name)\n",
    "        if isinstance(new_dict, pd.DataFrame):\n",
    "            new_dict = dict(new_dict)\n",
    "            new_dict[\"is_pd_dataframe\"] = True\n",
    "            is_pd_dataframe = True\n",
    "        else:\n",
    "            is_pd_dataframe = False\n",
    "        new_group = HDF_Object_Wrapper(\n",
    "            hdf_parent_file_name=self.hdf_parent_file_name,\n",
    "            hdf_parent_group_name=f\"{self.hdf_parent_group_name}/{name}\",\n",
    "            read_only=self.read_only,\n",
    "            truncate=self.truncate,\n",
    "        )\n",
    "        for key, value in new_dict.items():\n",
    "            new_group.__setattr__(key, value)\n",
    "        if is_pd_dataframe:\n",
    "            object.__setattr__(new_group, \"_dtype\", \"dataframe\")\n",
    "        object.__setattr__(self, \"_shape\", self._shape + 1)\n",
    "        return new_group\n",
    "    \n",
    "    def append(self, data):\n",
    "        if isinstance(self.dtype, str):\n",
    "            if self.dtype == \"dataframe\":\n",
    "                for column_name in self:\n",
    "                    array.append(data[column_name])\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "        else:\n",
    "            with h5py.File(self.hdf_parent_file_name, \"a\") as hdf_file:\n",
    "                hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "                new_shape = tuple([i + j for i, j in zip(self.shape, data.shape)])\n",
    "                hdf_object.resize(new_shape)\n",
    "                hdf_object[self.shape[0]: self.shape[0] + data.shape[0]] = data\n",
    "                object.__setattr__(self, \"_shape\", new_shape)\n",
    "    \n",
    "    def __getitem__(self, keys):\n",
    "        if not isinstance(self.dtype, str):\n",
    "            with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "                return hdf_file[self.hdf_parent_group_name][keys]\n",
    "        elif self.dtype == \"dataframe\":\n",
    "            with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "                hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "                arrays = {}\n",
    "                for name in hdf_object:\n",
    "                    if isinstance(hdf_object[name], h5py.Dataset):\n",
    "                        arrays[name] = hdf_object[name]\n",
    "                return pd.DataFrame(\n",
    "                    {\n",
    "                        name: array[keys] for name, array in arrays.items()\n",
    "                    }\n",
    "                )\n",
    "        elif self.dtype == \"group\":\n",
    "            with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "                hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "                if keys is Ellipsis:\n",
    "                    return {\n",
    "                        name: self.__getattribute__(name) for name in hdf_object\n",
    "                    }\n",
    "                elif keys not in hdf_object:\n",
    "                    raise KeyError(\n",
    "                        f\"No object with {keys} available in this group\"\n",
    "                    )\n",
    "                return self.__getattribute__(keys)\n",
    "            return object.self.__dict__[keys]  # TODO might be to generic?\n",
    "        else:\n",
    "            raise KeyError(\"Dtype not understood\")\n",
    "            \n",
    "            \n",
    "class HDF_File(HDF_Object_Wrapper):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hdf_parent_file_name: str,\n",
    "        *,\n",
    "        read_only: bool = True,\n",
    "        truncate: bool = False,\n",
    "        delete_existing: bool = False,\n",
    "    ):\n",
    "        if delete_existing:\n",
    "            mode = \"w\"\n",
    "        else:\n",
    "            mode = \"a\"\n",
    "        with h5py.File(hdf_parent_file_name, mode) as hdf_file:\n",
    "            pass\n",
    "        super().__init__(\n",
    "            hdf_parent_file_name=hdf_parent_file_name,\n",
    "            hdf_parent_group_name=\"/\",\n",
    "            read_only=read_only,\n",
    "            truncate=truncate,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:57:59.119927Z",
     "start_time": "2021-10-21T07:57:59.092064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   col2 col_str\n",
       " 0     0  b'str'\n",
       " 1     1    b'i'\n",
       " 2     2  b'ngs',\n",
       "    col2 col_str\n",
       " 0     0     str\n",
       " 1     1       i\n",
       " 2     2     ngs)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "\n",
    "import tempfile\n",
    "\n",
    "def test_HDF_wrappers():\n",
    "    hdf_file_name = os.path.join(tempfile.gettempdir(), \"sandbox.hdf\")\n",
    "    hdf_file = HDF_File(\n",
    "        hdf_file_name,\n",
    "        read_only=False,\n",
    "        truncate=True,\n",
    "        delete_existing=True\n",
    "    )\n",
    "    np.testing.assert_equal(0, hdf_file.shape)\n",
    "    file_size = os.path.getsize(hdf_file_name)\n",
    "    hdf_file.attr1 = 1\n",
    "    np.testing.assert_equal(1, hdf_file.attr1)\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    array = np.random.rand(10)\n",
    "    hdf_file.array = array\n",
    "    np.testing.assert_equal(array, hdf_file.array.values)\n",
    "    np.testing.assert_equal(array[:3], hdf_file.array[:3])\n",
    "    np.testing.assert_equal((10,), hdf_file.array.shape)\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    hdf_file.array.array_attr = \"some attr\"\n",
    "    np.testing.assert_equal(hdf_file.array.array_attr, \"some attr\")\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    group = {\n",
    "        \"subgroup1\": {\n",
    "            \"subsubgroup\": {},\n",
    "            \"same_array\": array,\n",
    "            \"a_bool\": True\n",
    "        },\n",
    "        \"subgroup2\": {}\n",
    "    }\n",
    "    hdf_file.group = group\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    np.testing.assert_equal(hdf_file.group.subgroup1.a_bool, True)\n",
    "    np.testing.assert_equal(hdf_file.group.subgroup1.shape, 2)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"col2\": np.arange(3),\n",
    "            \"col_str\": [\"str\", \"i\", \"ngs\"],\n",
    "        }\n",
    "    )\n",
    "    hdf_file.df = df\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    return hdf_file.df.values, df\n",
    "#     assert hdf_file.df.values.equals(df)\n",
    "    \n",
    "test_HDF_wrappers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:59:25.912271Z",
     "start_time": "2021-10-21T07:59:25.905285Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file_name = \"sandbox.hdf\"\n",
    "hdf_file = HDF_File(\n",
    "    hdf_file_name,\n",
    "    read_only=False,\n",
    "    truncate=True,\n",
    "    delete_existing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:59:26.250747Z",
     "start_time": "2021-10-21T07:59:26.244052Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file.attr1 = 1\n",
    "hdf_file.name = \"Mathhias\"\n",
    "hdf_file.group = {}\n",
    "hdf_file.group.array = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:59:26.598035Z",
     "start_time": "2021-10-21T07:59:26.594055Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file.group.array.append(np.arange(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:59:27.015129Z",
     "start_time": "2021-10-21T07:59:27.010135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf_file.group.array.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:59:27.504192Z",
     "start_time": "2021-10-21T07:59:27.494913Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file.df = pd.DataFrame(\n",
    "    {\n",
    "        \"feng\": np.arange(10),\n",
    "        \"sander\": \"my name\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T08:00:01.278658Z",
     "start_time": "2021-10-21T08:00:01.273045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_read_only': False,\n",
       " '_truncate': True,\n",
       " '_hdf_parent_file_name': 'sandbox.hdf',\n",
       " '_hdf_parent_group_name': '//df',\n",
       " '_dtype': 'dataframe',\n",
       " '_shape': 2,\n",
       " 'feng': <__main__.HDF_Object_Wrapper at 0x7fbd00ce1190>,\n",
       " 'sander': <__main__.HDF_Object_Wrapper at 0x7fbd00ce1460>}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf_file.df.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:45:46.407219Z",
     "start_time": "2021-10-21T07:45:46.403643Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"feng\": np.arange(10),\n",
    "        \"sander\": \"your name\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:47:47.607593Z",
     "start_time": "2021-10-21T07:47:47.601255Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file_name = \"sandbox.hdf\"\n",
    "hdf_file2 = HDF_File(\n",
    "    hdf_file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:48:51.369213Z",
     "start_time": "2021-10-21T07:48:51.365867Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file2.feng = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:47:59.462486Z",
     "start_time": "2021-10-21T07:47:59.458593Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file2.set_read_only(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:48:48.006689Z",
     "start_time": "2021-10-21T07:48:48.001908Z"
    }
   },
   "outputs": [],
   "source": [
    "hdf_file2.set_truncate(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:50:41.939854Z",
     "start_time": "2021-10-21T07:50:41.934518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attr1': 1, 'feng': 2, 'name': 'Mathhias'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf_file2.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:54:17.826531Z",
     "start_time": "2021-10-21T07:54:10.107297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502 µs ± 3.73 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "440 µs ± 25.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hdf_file.group.array[[1,5,8]]\n",
    "%timeit hdf_file.group.array.values[[1,5,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T15:31:50.266332Z",
     "start_time": "2021-10-15T15:31:50.230342Z"
    }
   },
   "outputs": [],
   "source": [
    "class HDF_Object_Wrapper(object):\n",
    "    '''\n",
    "    A generic class to access HDF components.\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        hdf_parent_file_name: str,\n",
    "        hdf_parent_group_name: str,\n",
    "        read_only: bool = True,\n",
    "        truncate: bool = False,\n",
    "    ):\n",
    "        self.set_read_only(read_only)\n",
    "        self.set_truncate(truncate)\n",
    "        object.__setattr__(self, \"_hdf_parent_file_name\", hdf_parent_file_name)\n",
    "        object.__setattr__(self, \"_hdf_parent_group_name\", hdf_parent_group_name)\n",
    "        self._import_from_hdf_file()\n",
    "    \n",
    "    def _import_from_hdf_file(self):\n",
    "        with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "            hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "            for name, value in hdf_object.attrs.items():\n",
    "                object.__setattr__(self, name, value)\n",
    "            for name in hdf_object:\n",
    "                if isinstance(hdf_object[name], h5py.Dataset):\n",
    "                    dataset = HDF_Dataset_Wrapper(\n",
    "                        hdf_parent_file_name=self.hdf_parent_file_name,\n",
    "                        hdf_parent_group_name=f\"{self.hdf_parent_group_name}/{name}\",\n",
    "                        read_only=self.read_only,\n",
    "                        truncate=self.truncate,\n",
    "                    )\n",
    "                    object.__setattr__(self, name, dataset)\n",
    "                else:\n",
    "                    subgroup = HDF_Object_Wrapper(\n",
    "                        hdf_parent_file_name=self.hdf_parent_file_name,\n",
    "                        hdf_parent_group_name=f\"{self.hdf_parent_group_name}/{name}\",\n",
    "                        read_only=self.read_only,\n",
    "                        truncate=self.truncate,\n",
    "                    )\n",
    "                    object.__setattr__(self, name, subgroup)\n",
    "    \n",
    "    def __setattr__(self, name, value):\n",
    "        if self.read_only:\n",
    "            raise AttributeError(\"Cannot set read-only attributes\")\n",
    "        elif not isinstance(name, str):\n",
    "            raise KeyError(f\"Attribute name '{name}' is not a string\")\n",
    "        elif not bool(re.match(r'^[a-zA-Z_][\\w.-]*$', test_string)):\n",
    "            raise KeyError(f\"Invalid attribute name: {name}\")\n",
    "        with h5py.File(self.hdf_parent_file_name, \"a\") as hdf_file:\n",
    "            hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "            exists = name in [\n",
    "                item for group in x.components for item in group\n",
    "            ]\n",
    "            if exists:\n",
    "                if not self.truncate:\n",
    "                    raise KeyError(\n",
    "                        f\"Attribute name '{name}' cannot be truncated\"\n",
    "                    )\n",
    "            if isinstance(value, (str, bool, int, float)):\n",
    "                hdf_object.attrs[name] = value\n",
    "                parsed_value = value\n",
    "            elif isinstance(value, (np.ndarray, pd.core.series.Series)):\n",
    "                if exists:\n",
    "                    del hdf_object[name]\n",
    "                parsed_value = self.__create_new_dataset(\n",
    "                    name,\n",
    "                    value,\n",
    "                    hdf_object\n",
    "                )\n",
    "            elif isinstance(value, (dict, pd.DataFrame)):\n",
    "                if exists:\n",
    "                    del hdf_object[name]\n",
    "                parsed_value = self.__create_new_group(\n",
    "                    name,\n",
    "                    value,\n",
    "                    hdf_object\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Invalid attribute value {value}\")\n",
    "            object.__setattr__(self, name, parsed_value)\n",
    "    \n",
    "    def __create_new_dataset(\n",
    "        self,\n",
    "        name:str,\n",
    "        array: np.ndarray,\n",
    "        hdf_object: h5py.Group,\n",
    "    ):    \n",
    "        if isinstance(array, (pd.core.series.Series)):\n",
    "            array = array.values\n",
    "        hdf_dataset = hdf_object.create_dataset(\n",
    "            name,\n",
    "            data=array,\n",
    "#             TODO\n",
    "#             compression=\"lzf\" if compress else None,\n",
    "#             # compression=\"gzip\" if compress else None, # TODO slower to make, faster to load?\n",
    "#             shuffle=compress,\n",
    "#             chunks=True if chunked else None,\n",
    "#             dtype=dtype\n",
    "        )\n",
    "        new_array = HDF_Dataset_Wrapper(\n",
    "            hdf_parent_file_name=self.hdf_parent_file_name,\n",
    "            hdf_parent_group_name=f\"{self.hdf_parent_group_name}/{name}\",\n",
    "            read_only=self.read_only,\n",
    "            truncate=self.truncate,\n",
    "        )\n",
    "        return new_array\n",
    "    \n",
    "    def __create_new_group(\n",
    "        self,\n",
    "        name:str,\n",
    "        new_dict: dict,\n",
    "        hdf_object: h5py.Group,\n",
    "    ):\n",
    "        if isinstance(new_dict, pd.DataFrame):\n",
    "            new_dict = dict(new_dict)\n",
    "            new_dict[\"is_pd_dataframe\"] = True\n",
    "        new_group = hdf_object.create_group(name)\n",
    "        new_object = HDF_Object_Wrapper(\n",
    "            hdf_parent_file_name=self.hdf_parent_file_name,\n",
    "            hdf_parent_group_name=f\"{self.hdf_parent_group_name}/{name}\",\n",
    "            read_only=self.read_only,\n",
    "            truncate=self.truncate,\n",
    "        )\n",
    "        for key, value in new_dict.items():\n",
    "            new_object.__setattr__(key, value)\n",
    "        return new_object\n",
    "\n",
    "    def set_read_only(self, read_only: bool = True):\n",
    "        object.__setattr__(self, \"_read_only\", read_only)\n",
    "        groups, datasets = self.components[:2]\n",
    "        for group in groups.values():\n",
    "            group.set_read_only(read_only)\n",
    "        for dataset in datasets.values():\n",
    "            dataset.set_read_only(read_only)\n",
    "        \n",
    "    def set_truncate(self, truncate: bool = True):\n",
    "        object.__setattr__(self, \"_truncate\", truncate)\n",
    "        groups, datasets = self.components[:2]\n",
    "        for group in groups.values():\n",
    "            group.set_truncate(truncate)\n",
    "        for dataset in datasets.values():\n",
    "            dataset.set_truncate(truncate)\n",
    "            \n",
    "    @property\n",
    "    def read_only(self):\n",
    "        return self._read_only\n",
    "    \n",
    "    @property\n",
    "    def truncate(self):\n",
    "        return self._truncate\n",
    "    \n",
    "    @property\n",
    "    def hdf_parent_file_name(self):\n",
    "        return self._hdf_parent_file_name\n",
    "    \n",
    "    @property\n",
    "    def hdf_parent_group_name(self):\n",
    "        return self._hdf_parent_group_name\n",
    "        \n",
    "    @property\n",
    "    def values(self):\n",
    "        if hasattr(self, \"is_pd_dataframe\") and self.is_pd_dataframe:\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    name: array.values for name, array in self.components[1].items()\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            return self.components[:2]\n",
    "        \n",
    "    @property\n",
    "    def metadata(self):\n",
    "        return self.components[2]\n",
    "    \n",
    "    @property\n",
    "    def components(self):\n",
    "        metadata = {}\n",
    "        groups = {}\n",
    "        datasets = {}\n",
    "        for key, value in self.__dict__.items():\n",
    "            if key[0] != \"_\":\n",
    "                if isinstance(value, HDF_Object_Wrapper):\n",
    "                    if isinstance(value, HDF_Dataset_Wrapper):\n",
    "                        datasets[key] = value\n",
    "                    else:\n",
    "                        groups[key] = value\n",
    "                else:\n",
    "                    metadata[key] = value\n",
    "        return (groups, datasets, metadata)\n",
    "        \n",
    "        \n",
    "class HDF_Dataset_Wrapper(HDF_Object_Wrapper):\n",
    "    '''\n",
    "    A generic class to access HDF Datasets.\n",
    "    '''\n",
    "    \n",
    "    def _import_from_hdf_file(self):\n",
    "        with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "            hdf_object = hdf_file[self.hdf_parent_group_name]\n",
    "            for name, value in hdf_object.attrs.items():\n",
    "                object.__setattr__(self, name, value)\n",
    "            object.__setattr__(self, \"_dtype\", hdf_object.dtype)\n",
    "            object.__setattr__(self, \"_shape\", hdf_object.shape)\n",
    "    \n",
    "    def __getitem__(self, keys):\n",
    "        with h5py.File(self.hdf_parent_file_name, \"r\") as hdf_file:\n",
    "            return hdf_file[self.hdf_parent_group_name][keys]\n",
    "\n",
    "    @property\n",
    "    def values(self):\n",
    "        return self[...]      \n",
    "     \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self._dtype\n",
    "        \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._shape\n",
    "    \n",
    "#     def set_read_only(self, read_only: bool = True):\n",
    "#         if not read_only:\n",
    "#             raise NotImplementedError(\"Datasets cannot be modified in-place.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T15:31:50.836210Z",
     "start_time": "2021-10-15T15:31:50.812148Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x = HDF_Object_Wrapper(\n",
    "    hdf_parent_file_name=\"/Users/swillems/Data/alphatims_testing2/20201016_tims03_Evo03_PS_MA_HeLa_200ng_DDA_06-15_5_6min_4cm_S1-A1_1_21717.hdf\",\n",
    "    hdf_parent_group_name=\"/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T15:31:51.262948Z",
     "start_time": "2021-10-15T15:31:51.260632Z"
    }
   },
   "outputs": [],
   "source": [
    "# %timeit x.raw._intensity_values\n",
    "# %timeit x.raw._intensity_values[::10]\n",
    "# %timeit x.raw._intensity_values.values\n",
    "# %timeit x.raw._intensity_values.values[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T15:31:51.727535Z",
     "start_time": "2021-10-15T15:31:51.700648Z"
    }
   },
   "outputs": [],
   "source": [
    "x.raw._frames.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T15:06:57.668869Z",
     "start_time": "2021-10-15T15:06:57.655327Z"
    }
   },
   "outputs": [],
   "source": [
    "x.set_read_only(False)\n",
    "x.set_truncate(True)\n",
    "# x.attr1 = \"YAR\"\n",
    "# x.dataset = np.arange(10)\n",
    "x.df2 = df\n",
    "x.components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T15:07:33.236105Z",
     "start_time": "2021-10-15T15:07:33.227587Z"
    }
   },
   "outputs": [],
   "source": [
    "x.df2.x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:15:23.255582Z",
     "start_time": "2021-10-15T13:15:23.248928Z"
    }
   },
   "outputs": [],
   "source": [
    "test_string = \"sanderÖ_1\"\n",
    "bool(re.match(r'^[a-zA-Z_][\\w.-]*$', test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T13:44:44.808126Z",
     "start_time": "2021-10-15T13:44:44.801000Z"
    }
   },
   "outputs": [],
   "source": [
    "[item for group in x.components for item in group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T14:16:22.822993Z",
     "start_time": "2021-10-15T14:16:22.814736Z"
    }
   },
   "outputs": [],
   "source": [
    "x.dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T14:22:48.931800Z",
     "start_time": "2021-10-15T14:22:48.923672Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": np.arange(10),\n",
    "        \"y\": np.arange(10) + 10,\n",
    "    }\n",
    ")\n",
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T14:33:46.354378Z",
     "start_time": "2021-10-15T14:33:46.349132Z"
    }
   },
   "outputs": [],
   "source": [
    "h5py.Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T12:18:26.453483Z",
     "start_time": "2021-10-15T12:18:21.955623Z"
    }
   },
   "outputs": [],
   "source": [
    "%timeit x.dataset[:3]\n",
    "%timeit x.dataset.values[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:39:00.610646Z",
     "start_time": "2021-10-15T09:39:00.606998Z"
    }
   },
   "outputs": [],
   "source": [
    "x._read_only = False\n",
    "# x.group.dataset.dtype = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:38:23.036034Z",
     "start_time": "2021-10-15T09:38:23.033783Z"
    }
   },
   "outputs": [],
   "source": [
    "x._hdf_parent_file_name = \"x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T12:20:45.911028Z",
     "start_time": "2021-10-15T12:20:45.907300Z"
    }
   },
   "outputs": [],
   "source": [
    "x.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T08:25:46.768211Z",
     "start_time": "2021-10-15T08:25:46.760508Z"
    }
   },
   "outputs": [],
   "source": [
    "x.group.sub_group.hdf_parent_group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T12:20:36.234876Z",
     "start_time": "2021-10-15T12:20:36.226303Z"
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"sandbox.hdf\", \"w\") as f:\n",
    "    f.attrs[\"file_attribute\"] = \"some_value\"\n",
    "    dataset = np.arange(5)\n",
    "    data = f.create_dataset(\"dataset\", data=dataset)\n",
    "#     help(data)\n",
    "    xx = data.__dict__\n",
    "    data.attrs[\"data_attribute\"] = \"some_value\"\n",
    "    group = f.create_group(\"group\")\n",
    "    group.attrs[\"group_attribute\"] = \"some_value\"\n",
    "    subdata = group.create_dataset(\"dataset\", data=dataset)\n",
    "    subdata.attrs[\"subdata_attribute\"] = \"some_value\"\n",
    "    subgroup = group.create_group(\"sub_group\")\n",
    "    subsubdata = subgroup.create_dataset(\"dataset\", data=dataset)\n",
    "    subsubdata.attrs[\"subsubdata_attribute\"] = \"some_value\"\n",
    "    subgroup.attrs[\"subgroup_attribute\"] = \"some_value\"\n",
    "\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:08:49.587253Z",
     "start_time": "2021-10-15T09:08:49.575536Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:58:12.597432Z",
     "start_time": "2021-10-15T07:58:12.589909Z"
    }
   },
   "outputs": [],
   "source": [
    "help(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:43:52.220116Z",
     "start_time": "2021-10-15T07:43:52.205962Z"
    }
   },
   "outputs": [],
   "source": [
    "# x._read_only=False\n",
    "x.y = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:39:37.156872Z",
     "start_time": "2021-10-15T07:39:37.152649Z"
    }
   },
   "outputs": [],
   "source": [
    "x.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T07:37:43.497539Z",
     "start_time": "2021-10-15T07:37:43.483052Z"
    }
   },
   "outputs": [],
   "source": [
    "help(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T16:17:34.193421Z",
     "start_time": "2021-10-14T16:17:34.189693Z"
    }
   },
   "outputs": [],
   "source": [
    "h5py.Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T13:13:14.247600Z",
     "start_time": "2021-10-14T13:13:14.233844Z"
    }
   },
   "outputs": [],
   "source": [
    "help(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T16:20:41.463726Z",
     "start_time": "2021-10-14T16:20:41.460061Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "  \n",
    "class HDFGroup(h5py.Group):\n",
    "    \n",
    "    def create_dataset(self, name, shape=None, dtype=None, data=None, **kwds):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            print(\"dataframe\")\n",
    "            super().create_dataset(name, shape=shape, dtype=dtype, data=data, **kwds) \n",
    "        else:\n",
    "            super().create_dataset(name, shape=shape, dtype=dtype, data=data, **kwds)\n",
    "            \n",
    "class HDFFile(h5py.File, HDFGroup): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T16:26:15.627862Z",
     "start_time": "2021-10-14T16:26:15.619692Z"
    }
   },
   "outputs": [],
   "source": [
    "with HDFFile(\"sandbox.hdf\", \"w\") as f:\n",
    "    x = np.arange(5)\n",
    "    f.create_dataset(\"x\", data=x)\n",
    "    d = {\n",
    "            \"zz\": x,\n",
    "            \"x\": x.astype(np.int64),\n",
    "            \"xx\": x.astype(np.float64),\n",
    "    #         \"z\": [\"sander\", \"2\", \"23\", \"4\", \"five\"],\n",
    "        }\n",
    "    y=pd.DataFrame(\n",
    "        d,\n",
    "        copy=False\n",
    "    )\n",
    "    y[\"z\"] = x\n",
    "    f.create_dataset(\"y\", data=y)\n",
    "    yy = f[\"y\"][...]\n",
    "    g = f.create_group(\"g\")\n",
    "    g.create_dataset(\"y\", data=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T16:25:57.717718Z",
     "start_time": "2021-10-14T16:25:57.711016Z"
    }
   },
   "outputs": [],
   "source": [
    "type(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T14:40:29.586713Z",
     "start_time": "2021-10-14T14:40:29.583926Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T14:40:30.066600Z",
     "start_time": "2021-10-14T14:40:30.063560Z"
    }
   },
   "outputs": [],
   "source": [
    "f.create_dataset(\"test\", data=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T15:52:21.414175Z",
     "start_time": "2021-10-14T15:52:21.408744Z"
    }
   },
   "outputs": [],
   "source": [
    "y=pd.DataFrame(\n",
    "    {\n",
    "        \"x\": x.astype(np.int64),\n",
    "        \"x\": x.astype(np.float64),\n",
    "#         \"z\": [\"sander\", \"2\", \"23\", \"4\", \"five\"],\n",
    "    },\n",
    "    copy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T14:40:40.067991Z",
     "start_time": "2021-10-14T14:40:40.042759Z"
    }
   },
   "outputs": [],
   "source": [
    "f.create_dataset(\"test2\", data=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T14:40:46.292745Z",
     "start_time": "2021-10-14T14:40:46.290153Z"
    }
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T15:56:54.649810Z",
     "start_time": "2021-10-14T15:56:54.638125Z"
    }
   },
   "outputs": [],
   "source": [
    "x[0]=2\n",
    "d[\"zz\"][0] = 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T16:20:50.825224Z",
     "start_time": "2021-10-14T16:20:50.822237Z"
    }
   },
   "outputs": [],
   "source": [
    "# f = h5py.File(\"sandbox2.hdf\", \"w\")\n",
    "f = HDFFile(\"sandbox5.hdf\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T16:20:51.151829Z",
     "start_time": "2021-10-14T16:20:51.144326Z"
    }
   },
   "outputs": [],
   "source": [
    "help(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T16:25:32.078225Z",
     "start_time": "2021-10-14T16:25:32.066698Z"
    }
   },
   "outputs": [],
   "source": [
    "help(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "from alphapept.__main__ import VERSION_NO\n",
    "\n",
    "\n",
    "class HDF_File(object):\n",
    "    '''\n",
    "    A generic class to store and retrieve on-disk\n",
    "    data with an HDF container.\n",
    "    '''\n",
    "\n",
    "    @property\n",
    "    def original_file_name(self):\n",
    "        return self.read(\n",
    "            attr_name=\"original_file_name\"\n",
    "        )  # See below for function definition\n",
    "\n",
    "    @property\n",
    "    def file_name(self):\n",
    "        return self.__file_name\n",
    "\n",
    "    @property\n",
    "    def directory(self):\n",
    "        return os.path.dirname(self.file_name)\n",
    "\n",
    "    @property\n",
    "    def creation_time(self):\n",
    "        return self.read(\n",
    "            attr_name=\"creation_time\"\n",
    "        )  # See below for function definition\n",
    "\n",
    "    @property\n",
    "    def last_updated(self):\n",
    "        return self.read(\n",
    "            attr_name=\"last_updated\"\n",
    "        )  # See below for function definition\n",
    "\n",
    "    @property\n",
    "    def version(self):\n",
    "        return self.read(\n",
    "            attr_name=\"version\"\n",
    "        )  # See below for function definition\n",
    "\n",
    "    @property\n",
    "    def is_read_only(self):\n",
    "        return self.__is_read_only\n",
    "\n",
    "    @property\n",
    "    def is_overwritable(self):\n",
    "        return self.__is_overwritable\n",
    "\n",
    "    def read(self):\n",
    "        pass\n",
    "\n",
    "    def write(self):\n",
    "        pass\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_name: str,\n",
    "        is_read_only: bool = True,\n",
    "        is_new_file: bool = False,\n",
    "        is_overwritable: bool = False,\n",
    "    ):\n",
    "        \"\"\"Create/open a wrapper object to access HDF data.\n",
    "\n",
    "        Args:\n",
    "            file_name (str): The file_name of the HDF file.\n",
    "            is_read_only (bool): If True, the HDF file cannot be modified. Defaults to True.\n",
    "            is_new_file (bool): If True, an already existing file will be completely removed. Defaults to False.\n",
    "            is_overwritable (bool): If True, already existing arrays will be overwritten. If False, only new data can be appended. Defaults to False.\n",
    "\n",
    "        \"\"\"\n",
    "        self.__file_name = os.path.abspath(file_name)\n",
    "        if is_new_file:\n",
    "            is_read_only = False\n",
    "            if not os.path.exists(self.directory):\n",
    "                os.makedirs(self.directory)\n",
    "            with h5py.File(self.file_name, \"w\") as hdf_file:\n",
    "                current_time = time.asctime()\n",
    "                hdf_file.attrs[\"creation_time\"] = current_time\n",
    "                hdf_file.attrs[\"original_file_name\"] = self.__file_name\n",
    "                hdf_file.attrs[\"version\"] = VERSION_NO\n",
    "                hdf_file.attrs[\"last_updated\"] = current_time\n",
    "        else:\n",
    "            with h5py.File(self.file_name, \"r\") as hdf_file:\n",
    "                self.check()\n",
    "        if is_overwritable:\n",
    "            is_read_only = False\n",
    "        self.__is_read_only = is_read_only\n",
    "        self.__is_overwritable = is_overwritable\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.file_name == other.file_name\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.file_name)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<HDF_File {self.file_name}>\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def check(\n",
    "        self,\n",
    "        version: bool = True,\n",
    "        file_name: bool = True,\n",
    "    ) -> list:\n",
    "        \"\"\"Check if the `version` or `file_name` of this HDF_File have changed.\n",
    "\n",
    "        Args:\n",
    "            version (bool): If False, do not check the version. Defaults to True.\n",
    "            file_name (bool): If False, do not check the file_name. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of warning messages stating any issues.\n",
    "\n",
    "        \"\"\"\n",
    "        warning_messages = []\n",
    "        if version:\n",
    "            current_version = VERSION_NO\n",
    "            creation_version = self.version\n",
    "            if creation_version != current_version:\n",
    "                warning_messages.append(\n",
    "                    f\"{self} was created with version \"\n",
    "                    f\"{creation_version} instead of {current_version}.\"\n",
    "                )\n",
    "        if file_name:\n",
    "            if self.file_name != self.original_file_name:\n",
    "                warning_messages.append(\n",
    "                    f\"The file name of {self} has been changed from\"\n",
    "                    f\"{self.original_file_name} to {self.file_name}.\"\n",
    "                )\n",
    "        return warning_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T16:00:07.359553Z",
     "start_time": "2021-10-18T16:00:07.350190Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T16:00:30.709163Z",
     "start_time": "2021-10-18T16:00:30.703496Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T18:29:26.940897Z",
     "start_time": "2021-10-19T18:29:26.937559Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# x = HDF_Object_Wrapper(\n",
    "#     hdf_parent_file_name=\"/Users/swillems/Data/alphatims_testing2/20201016_tims03_Evo03_PS_MA_HeLa_200ng_DDA_06-15_5_6min_4cm_S1-A1_1_21717.hdf\",\n",
    "#     hdf_parent_group_name=\"/\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T18:31:05.470397Z",
     "start_time": "2021-10-19T18:31:05.465861Z"
    }
   },
   "outputs": [],
   "source": [
    "x = HDF_File(\"sanderbox.hdf\", read_only=False, truncate=True, new=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T18:30:34.562407Z",
     "start_time": "2021-10-19T18:30:34.556345Z"
    }
   },
   "outputs": [],
   "source": [
    "x.group = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T18:30:34.975101Z",
     "start_time": "2021-10-19T18:30:34.968018Z"
    }
   },
   "outputs": [],
   "source": [
    "x.group_with_att = {\n",
    "    \"name\": \"sander\",\n",
    "    \"number\": 1,\n",
    "    \"is_true\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T18:30:35.286096Z",
     "start_time": "2021-10-19T18:30:35.281423Z"
    }
   },
   "outputs": [],
   "source": [
    "x.group.data = np.arange(10) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T18:30:35.833778Z",
     "start_time": "2021-10-19T18:30:35.823188Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"col_str\": [\"str\", \"i\", \"ngs\"],\n",
    "        \"col2\": np.empty(3)\n",
    "    }\n",
    ")\n",
    "x.df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T18:29:57.086771Z",
     "start_time": "2021-10-19T18:29:57.075581Z"
    }
   },
   "outputs": [],
   "source": [
    "x.df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T18:29:52.080347Z",
     "start_time": "2021-10-19T18:29:52.073955Z"
    }
   },
   "outputs": [],
   "source": [
    "x.set_read_only(True)\n",
    "# x.set_truncate(True)\n",
    "# x.raw.test = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T10:24:15.591374Z",
     "start_time": "2021-10-19T10:24:15.587795Z"
    }
   },
   "outputs": [],
   "source": [
    "isinstance(x.raw._mz_values.dtype, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T10:23:38.203993Z",
     "start_time": "2021-10-19T10:23:38.163239Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in x.raw._mobility_values: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T10:23:01.327672Z",
     "start_time": "2021-10-19T10:23:01.321401Z"
    }
   },
   "outputs": [],
   "source": [
    "x.raw._mz_values.hdf_parent_group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alphabase]",
   "language": "python",
   "name": "conda-env-alphabase-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
