{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module provides a common interface to access HDF files. It can be imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphabase.io.hdf\n",
    "\n",
    "# Other packages used to demonstrate functionality\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of relying directly on the `h5py` interface, we will use an HDF wrapper file to provide consistent access to only those specific HDF features we want. Since components of an HDF file come in three shapes `datasets`, `groups` and `attributes`, we will first define a generic HDF wrapper object to handle these components. Once this is done, the HDF wrapper file can be treated as such an object with additional features to open and close the initial connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "TEMPDIR = tempfile.gettempdir()\n",
    "\n",
    "hdf_file_name = os.path.join(TEMPDIR, \"sandbox.hdf\")\n",
    "hdf_file = alphabase.io.hdf.HDF_File(\n",
    "    hdf_file_name,\n",
    "    read_only=False,\n",
    "    truncate=True,\n",
    "    delete_existing=True\n",
    ")\n",
    "np.testing.assert_equal(len(hdf_file), 0)\n",
    "file_size = os.path.getsize(hdf_file_name)\n",
    "hdf_file.attr1 = 1\n",
    "np.testing.assert_equal(hdf_file.attr1, 1)\n",
    "file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "assert file_size > old_file_size, \"Filesize not increased\"\n",
    "np.random.seed(42)\n",
    "array = np.random.rand(10)\n",
    "hdf_file.array = array\n",
    "np.testing.assert_equal(array, hdf_file.array.values)\n",
    "np.testing.assert_equal(array[:3], hdf_file.array[:3])\n",
    "np.testing.assert_equal((10,), hdf_file.array.shape)\n",
    "file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "assert file_size > old_file_size, \"Filesize not increased\"\n",
    "hdf_file.array.array_attr = \"some attr\"\n",
    "np.testing.assert_equal(hdf_file.array.array_attr, \"some attr\")\n",
    "file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "assert file_size > old_file_size, \"Filesize not increased\"\n",
    "group = {\n",
    "    \"subgroup1\": {\n",
    "        \"subsubgroup\": {},\n",
    "        \"same_array\": array,\n",
    "        \"a_bool\": True\n",
    "    },\n",
    "    \"subgroup2\": {}\n",
    "}\n",
    "hdf_file.group = group\n",
    "file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "assert file_size > old_file_size, \"Filesize not increased\"\n",
    "np.testing.assert_equal(hdf_file.group.subgroup1.a_bool, True)\n",
    "np.testing.assert_equal(len(hdf_file.group.subgroup1), 2)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"col2\": np.arange(3),\n",
    "        \"col_str\": [\"str\", \"i\", \"ngs\"],\n",
    "    }\n",
    ")\n",
    "hdf_file.dfs = {}\n",
    "hdf_file.dfs.df_df = df\n",
    "hdf_file.dfs.df_df.hash_seed = 1337\n",
    "hdf_file.dfs.df_df.data_from = \"colleagues\"\n",
    "file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "assert file_size > old_file_size, \"Filesize not increased\"\n",
    "assert hdf_file.dfs.df_df.values.equals(df)\n",
    "\n",
    "try:\n",
    "    hdf_file.dfs.df = df\n",
    "except TypeError as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "hdf_file_name = os.path.join(TEMPDIR, \"sandbox.hdf\")\n",
    "hdf_file = alphabase.io.hdf.HDF_File(\n",
    "    hdf_file_name,\n",
    ")\n",
    "np.testing.assert_equal(hdf_file.attr1, 1)\n",
    "np.random.seed(42)\n",
    "array = np.random.rand(10)\n",
    "np.testing.assert_equal(array, hdf_file.array.values)\n",
    "np.testing.assert_equal(array[:3], hdf_file.array[:3])\n",
    "np.testing.assert_equal((10,), hdf_file.array.shape)\n",
    "np.testing.assert_equal(hdf_file.array.array_attr, \"some attr\")\n",
    "np.testing.assert_equal(hdf_file.group.subgroup1.a_bool, True)\n",
    "np.testing.assert_equal(len(hdf_file.group.subgroup1), 2)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"col2\": np.arange(3),\n",
    "        \"col_str\": [\"str\", \"i\", \"ngs\"],\n",
    "    }\n",
    ")\n",
    "assert hdf_file.dfs.df_df.hash_seed == 1337\n",
    "assert hdf_file.dfs.df_df.data_from == \"colleagues\"\n",
    "assert hdf_file.dfs.df_df.values.equals(df)\n",
    "assert hdf_file.dfs.__getattribute__(\"df_df\").values.equals(df)\n",
    "assert hdf_file.__getattribute__('dfs').__getattribute__(\"df_df\").values.equals(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
