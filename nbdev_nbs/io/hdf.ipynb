{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T11:42:03.508456Z",
     "start_time": "2021-10-27T11:42:03.485066Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module provides a common interface to access HDF files. It can be imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T11:42:04.387898Z",
     "start_time": "2021-10-27T11:42:03.510566Z"
    }
   },
   "outputs": [],
   "source": [
    "import alphabase.io.hdf\n",
    "\n",
    "# Other packages used to demonstrate functionality\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of relying directly on the `h5py` interface, we will use an HDF wrapper file to provide consistent access to only those specific HDF features we want. Since components of an HDF file come in three shapes `datasets`, `groups` and `attributes`, we will first define a generic HDF wrapper object to handle these components. Once this is done, the HDF wrapper file can be treated as such an object with additional features to open and close the initial connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T11:42:04.465240Z",
     "start_time": "2021-10-27T11:42:04.390507Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swillems/Documents/software/alphabase/alphabase/io/hdf.py:399: UserWarning: swmr=True only affects read ('r') mode. For swmr write mode, set f.swmr_mode = True after opening the file.\n",
      "  with h5py.File(file_name, mode, swmr=True):\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "\n",
    "import tempfile\n",
    "TEMPDIR = tempfile.gettempdir()\n",
    "\n",
    "def test_HDF_creation():\n",
    "    hdf_file_name = os.path.join(TEMPDIR, \"sandbox.hdf\")\n",
    "    hdf_file = alphabase.io.hdf.HDF_File(\n",
    "        hdf_file_name,\n",
    "        read_only=False,\n",
    "        truncate=True,\n",
    "        delete_existing=True\n",
    "    )\n",
    "    np.testing.assert_equal(len(hdf_file), 0)\n",
    "    file_size = os.path.getsize(hdf_file_name)\n",
    "    hdf_file.attr1 = 1\n",
    "    np.testing.assert_equal(hdf_file.attr1, 1)\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    np.random.seed(42)\n",
    "    array = np.random.rand(10)\n",
    "    hdf_file.array = array\n",
    "    np.testing.assert_equal(array, hdf_file.array.values)\n",
    "    np.testing.assert_equal(array[:3], hdf_file.array[:3])\n",
    "    np.testing.assert_equal((10,), hdf_file.array.shape)\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    hdf_file.array.array_attr = \"some attr\"\n",
    "    np.testing.assert_equal(hdf_file.array.array_attr, \"some attr\")\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    group = {\n",
    "        \"subgroup1\": {\n",
    "            \"subsubgroup\": {},\n",
    "            \"same_array\": array,\n",
    "            \"a_bool\": True\n",
    "        },\n",
    "        \"subgroup2\": {}\n",
    "    }\n",
    "    hdf_file.group = group\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    np.testing.assert_equal(hdf_file.group.subgroup1.a_bool, True)\n",
    "    np.testing.assert_equal(len(hdf_file.group.subgroup1), 2)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"col2\": np.arange(3),\n",
    "            \"col_str\": [\"str\", \"i\", \"ngs\"],\n",
    "        }\n",
    "    )\n",
    "    hdf_file.df = df\n",
    "    file_size, old_file_size = os.path.getsize(hdf_file_name), file_size\n",
    "    assert file_size > old_file_size, \"Filesize not increased\"\n",
    "    assert hdf_file.df.values.equals(df)\n",
    "    \n",
    "test_HDF_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T11:42:04.504530Z",
     "start_time": "2021-10-27T11:42:04.467892Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "def test_HDF_reading():\n",
    "    hdf_file_name = os.path.join(TEMPDIR, \"sandbox.hdf\")\n",
    "    hdf_file = alphabase.io.hdf.HDF_File(\n",
    "        hdf_file_name,\n",
    "    )\n",
    "    np.testing.assert_equal(hdf_file.attr1, 1)\n",
    "    np.random.seed(42)\n",
    "    array = np.random.rand(10)\n",
    "    np.testing.assert_equal(array, hdf_file.array.values)\n",
    "    np.testing.assert_equal(array[:3], hdf_file.array[:3])\n",
    "    np.testing.assert_equal((10,), hdf_file.array.shape)\n",
    "    np.testing.assert_equal(hdf_file.array.array_attr, \"some attr\")\n",
    "    np.testing.assert_equal(hdf_file.group.subgroup1.a_bool, True)\n",
    "    np.testing.assert_equal(len(hdf_file.group.subgroup1), 2)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"col2\": np.arange(3),\n",
    "            \"col_str\": [\"str\", \"i\", \"ngs\"],\n",
    "        }\n",
    "    )\n",
    "    assert hdf_file.df.values.equals(df)\n",
    "    \n",
    "    \n",
    "test_HDF_reading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic usage:\n",
    "> ```\n",
    "> #Create a new hdf file and write data to it\n",
    "> hdf_file = alphabase.io.hdf.HDF_File('test.hdf', read_only = False)\n",
    "> hdf_file.a = np.array([1,2,3])\n",
    "> hdf_file.b = np.array([4,5,6])\n",
    "> \n",
    "> #Read data, show components and access data:\n",
    "> hdf_file = alphabase.io.hdf.HDF_File('test.hdf')\n",
    "> print(hdf_file.components)\n",
    "> print(hdf_file.a.values)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alphabase]",
   "language": "python",
   "name": "conda-env-alphabase-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
