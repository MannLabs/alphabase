{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scoring.ml_scoring_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Class of ML Scoring Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from alphabase.scoring.feature_extraction_base import BaseFeatureExtractor\n",
    "from alphabase.scoring.fdr import (\n",
    "    calculate_fdr,\n",
    "    calculate_fdr_from_ref,\n",
    "    fdr_to_q_values,\n",
    "    fdr_from_ref,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two key modules in ML-based rescoring: feature extraction and rescoring algorithm. Here we designed these two modules as flexible as possible for future extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "The feature extractor is more important than the ML methods, so we designed a flexible architecture for feature extraction. As shown in `BaseFeatureExtractor`, a feature extractor inherited from `BaseFeatureExtractor` must re-implement `BaseFeatureExtractor.extract_features`, and tells the ML methods what are the extracted features by providing `BaseFeatureExtractor.feature_list`. \n",
    "\n",
    "For example, if we have two feature extractors, `AlphaPeptFE` and `AlphaPeptDeepFE`:\n",
    "\n",
    "```python\n",
    "class AlphaPeptFE(BaseFeatureExtractor):\n",
    "    def extract_features(self, psm_df):\n",
    "        psm_df['ap_f1'] = ...\n",
    "        self._feature_list.append('ap_f1')\n",
    "        psm_df['ap_f2'] = ...\n",
    "        self._feature_list.append('ap_f2')\n",
    "\n",
    "class AlphaPeptDeepFE(BaseFeatureExtractor):\n",
    "    def extract_features(self, psm_df):\n",
    "        psm_df['ad_f1'] = ...\n",
    "        self._feature_list.append('ad_f1')\n",
    "        psm_df['ad_f2'] = ...\n",
    "        self._feature_list.append('ad_f2')\n",
    "```\n",
    "\n",
    "We can easily design a new feature extractor which combines these two and more feature extractors:\n",
    "\n",
    "```python\n",
    "class CombFE(BaseFeatureExtractor):\n",
    "    def __init__(self):\n",
    "        self.fe_list = [AlphaPeptFE(),AlphaPeptDeepFE()]\n",
    "\n",
    "    def extract_features(self, psm_df):\n",
    "        for fe in self.fe_list:\n",
    "            fe.extract_features(psm_df)\n",
    "\n",
    "    @property\n",
    "    def feature_list(self):\n",
    "        f_set = set()\n",
    "        for fe in self.fe_list:\n",
    "            f_set.update(fe.feature_list)\n",
    "        return list(f_set)\n",
    "```\n",
    "\n",
    "This will be useful for rescoring with DL features, for instance, when AlphaPeptDeep is or is not installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescoring Algorithm\n",
    "\n",
    "The rescoring algorithm called `Percolator` (Kall et al. 2007) based on the semi-supervised learning algorithm is still the most widely used in MS-based proteomics. Therefore, we used `Percolator` as the base rescoring class and others can re-implement its methods for different algorithms.  as well as different \n",
    "\n",
    "1. Rescoring algorithm. We have provided the base rescoring code structure in `Percolator`. If we are going to support DiaNN's brute-force supervised learning methods, we can define the class like this:\n",
    "\n",
    "```python\n",
    "class DiaNNRescoring(Percolator):\n",
    "    def _train(self, train_t_df, train_d_df):\n",
    "        # No target filtration on FDR, which is the same as DiaNN but different in Percolator\n",
    "        #train_t_df = train_t_df[train_t_df.fdr<=self.fdr]\n",
    "        train_df = pd.concat((train_t_df, train_d_df))\n",
    "        train_label = np.ones(len(train_df),dtype=np.int32)\n",
    "        train_label[len(train_t_df):] = 0\n",
    "\n",
    "        self._ml_model.fit(\n",
    "            train_df[self.feature_list].values, \n",
    "            train_label\n",
    "        )\n",
    "    def rescore(self, psm_df):\n",
    "        # We don't need iteration anymore, but cross validation may be still necessary\n",
    "        df = self._cv_score(df)\n",
    "        return self._estimate_fdr(df)\n",
    "```\n",
    "\n",
    "2. ML models. Personally, `Percolator` with a linear classifier (SVM or LogisticRegression) is prefered. But as a framework, we should support different ML models. We can easily switch to the random forest by `self.ml_model = RandomForestClassifier()`. We can also use a DL model which provides sklearn-like `fit()` and `decision_function()` APIs for rescoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Percolator:\n",
    "    def __init__(self):\n",
    "        self._feature_extractor:BaseFeatureExtractor = BaseFeatureExtractor()\n",
    "        self._ml_model = LogisticRegression()\n",
    "        \n",
    "        self.fdr_level = 'psm' # psm, precursor, peptide, or sequence\n",
    "        self.fdr = 0.01\n",
    "        self.per_raw_fdr = False\n",
    "\n",
    "        self.max_training_sample = 200000\n",
    "        self.min_training_sample = 100\n",
    "        self.cv_fold = 1\n",
    "        self.iter_num = 1\n",
    "\n",
    "    @property\n",
    "    def feature_list(self)->list:\n",
    "        \"\"\" The read-only property to get extracted feature_list \"\"\"\n",
    "        return self.feature_extractor.feature_list\n",
    "\n",
    "    @property\n",
    "    def ml_model(self):\n",
    "        return self._ml_model\n",
    "    \n",
    "    @ml_model.setter\n",
    "    def ml_model(self, model):\n",
    "        \"\"\" \n",
    "        `model` must be sklearn models or other models but implement \n",
    "        the same methods `fit()` and `decision_function()`/`predict_proba()` \n",
    "        as sklearn models\n",
    "        \"\"\"\n",
    "        self._ml_model = model\n",
    "\n",
    "    @property\n",
    "    def feature_extractor(self)->BaseFeatureExtractor:\n",
    "        return self._feature_extractor\n",
    "    \n",
    "    @feature_extractor.setter\n",
    "    def feature_extractor(self, fe:BaseFeatureExtractor):\n",
    "        self._feature_extractor = fe\n",
    "\n",
    "    def extract_features(self,\n",
    "        psm_df:pd.DataFrame,\n",
    "        *args, **kwargs\n",
    "    )->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract features for rescoring.\n",
    "\n",
    "        *args and **kwargs are used for \n",
    "        `self.feature_extractor.extract_features`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        psm_df : pd.DataFrame\n",
    "            PSM DataFrame\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            psm_df with feature columns appended inplace.\n",
    "        \"\"\"\n",
    "        psm_df['ml_score'] = psm_df.score\n",
    "        psm_df = self._estimate_psm_fdr(psm_df)\n",
    "        return self._feature_extractor.extract_features(\n",
    "            psm_df, *args, **kwargs\n",
    "        )\n",
    "\n",
    "    def rescore(self, \n",
    "        df:pd.DataFrame\n",
    "    )->pd.DataFrame:\n",
    "        \"\"\"Rescore\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            psm_df\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            psm_df with `ml_score` and `fdr` columns updated inplace\n",
    "        \"\"\"\n",
    "        for i in range(self.iter_num):\n",
    "            df = self._cv_score(df)\n",
    "            df = self._estimate_fdr(df, 'psm', False)\n",
    "        df = self._estimate_fdr(df)\n",
    "        return df\n",
    "\n",
    "    def run(self,\n",
    "        psm_df:pd.DataFrame,\n",
    "        *args, **kwargs\n",
    "    )->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run percolator workflow:\n",
    "\n",
    "        - self.extract_features()\n",
    "        - self.re_score()\n",
    "\n",
    "        *args and **kwargs are used for \n",
    "        `self.feature_extractor.extract_features`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        psm_df : pd.DataFrame\n",
    "            PSM DataFrame\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            psm_df with feature columns appended inplace.\n",
    "        \"\"\"\n",
    "        df = self.extract_features(\n",
    "            psm_df, *args, **kwargs\n",
    "        )\n",
    "        return self.rescore(df)\n",
    "\n",
    "    def _estimate_fdr_per_raw(self,\n",
    "        df:pd.DataFrame,\n",
    "        fdr_level:str\n",
    "    )->pd.DataFrame:\n",
    "        df_list = []\n",
    "        for raw_name, df_raw in df.groupby('raw_name'):\n",
    "            df_list.append(self._estimate_fdr(df_raw, \n",
    "                fdr_level = fdr_level,\n",
    "                per_raw_fdr = False\n",
    "            ))\n",
    "        return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    def _estimate_psm_fdr(self,\n",
    "        df:pd.DataFrame,\n",
    "    )->pd.DataFrame:\n",
    "        df = df.sort_values(\n",
    "            ['ml_score','decoy'], ascending=False\n",
    "        ).reset_index(drop=True)\n",
    "        target_values = 1-df['decoy'].values\n",
    "        decoy_cumsum = np.cumsum(df['decoy'].values)\n",
    "        target_cumsum = np.cumsum(target_values)\n",
    "        fdr_values = decoy_cumsum/target_cumsum\n",
    "        df['fdr'] = fdr_to_q_values(fdr_values)\n",
    "        return df\n",
    "        \n",
    "    def _estimate_fdr(self, \n",
    "        df:pd.DataFrame,\n",
    "        fdr_level:str=None,\n",
    "        per_raw_fdr:bool=None,\n",
    "    )->pd.DataFrame:\n",
    "        if fdr_level is None: \n",
    "            fdr_level = self.fdr_level\n",
    "        if per_raw_fdr is None: \n",
    "            per_raw_fdr = self.per_raw_fdr\n",
    "\n",
    "        if per_raw_fdr:\n",
    "            return self._estimate_fdr_per_raw(\n",
    "                df, fdr_level=fdr_level\n",
    "            )\n",
    "\n",
    "        if fdr_level == 'psm':\n",
    "            return self._estimate_psm_fdr(df)\n",
    "        else:\n",
    "            if fdr_level == 'precursor':\n",
    "                _df = df.groupby([\n",
    "                    'sequence','mods','mod_sites','charge','decoy'\n",
    "                ])['ml_score'].max()\n",
    "            elif fdr_level == 'peptide':\n",
    "                _df = df.groupby([\n",
    "                    'sequence','mods','mod_sites','decoy'\n",
    "                ])['ml_score'].max()\n",
    "            else:\n",
    "                _df = df.groupby(['sequence','decoy'])['ml_score'].max()\n",
    "            _df = self._estimate_psm_fdr(_df)\n",
    "            df['fdr'] = fdr_from_ref(\n",
    "                df['ml_score'].values, _df['ml_score'].values, \n",
    "                _df['fdr'].values\n",
    "            )\n",
    "        return df\n",
    "\n",
    "    def _train(self, \n",
    "        train_t_df:pd.DataFrame, \n",
    "        train_d_df:pd.DataFrame\n",
    "    ):\n",
    "        train_t_df = train_t_df[train_t_df.fdr<=self.fdr]\n",
    "\n",
    "        if len(train_t_df) > self.max_train_sample:\n",
    "            train_t_df = train_t_df.sample(\n",
    "                n=self.max_training_sample, \n",
    "                random_state=1337\n",
    "            )\n",
    "        if len(train_d_df) > self.max_train_sample:\n",
    "            train_d_df = train_d_df.sample(\n",
    "                n=self.max_training_sample,\n",
    "                random_state=1337\n",
    "            )\n",
    "\n",
    "        train_df = pd.concat((train_t_df, train_d_df))\n",
    "        train_label = np.ones(len(train_df),dtype=np.int32)\n",
    "        train_label[len(train_t_df):] = 0\n",
    "\n",
    "        self._ml_model.fit(\n",
    "            train_df[self.feature_list].values, \n",
    "            train_label\n",
    "        )\n",
    "\n",
    "    def _predict(self, test_df):\n",
    "        try:\n",
    "            test_df['ml_score'] = self._ml_model.decision_function(\n",
    "                test_df[self.feature_list].values\n",
    "            )\n",
    "        except AttributeError:\n",
    "            test_df['ml_score'] = self._ml_model.predict_proba(\n",
    "                test_df[self.feature_list].values\n",
    "            )\n",
    "        return test_df\n",
    "\n",
    "    def _cv_score(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply cross-validation for rescoring.\n",
    "\n",
    "        It will split `df` into K folds. For each fold, \n",
    "        its ML scores are predicted by a model which \n",
    "        is trained by other K-1 folds .\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            PSMs to be rescored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            PSMs after rescoring\n",
    "        \"\"\"\n",
    "        df = df.sample(\n",
    "            frac=1, random_state=1337\n",
    "        ).reset_index(drop=True)\n",
    "        df_target = df[df.decoy == 0]\n",
    "        df_decoy = df[df.decoy != 0]\n",
    "\n",
    "        if (\n",
    "            np.sum(df_target.fdr<self.fdr) < \n",
    "            self.min_training_sample*self.cv_fold \n",
    "            or len(df_decoy) < \n",
    "            self.min_training_sample*self.cv_fold\n",
    "        ):\n",
    "            return df\n",
    "        \n",
    "        if self.cv_fold > 1:\n",
    "            test_df_list = []\n",
    "            for i in range(self.cv_fold):\n",
    "                t_mask = np.ones(len(df_target), dtype=bool)\n",
    "                _slice = slice(i, len(df_target), self.cv_fold)\n",
    "                t_mask[_slice] = False\n",
    "                train_t_df = df_target[t_mask]\n",
    "                test_t_df = df_target[_slice]\n",
    "                \n",
    "                d_mask = np.ones(len(df_decoy), dtype=bool)\n",
    "                _slice = slice(i, len(df_decoy), self.cv_fold)\n",
    "                d_mask[_slice] = False\n",
    "                train_d_df = df_decoy[d_mask]\n",
    "                test_d_df = df_decoy[_slice]\n",
    "\n",
    "                self._train(train_t_df, train_d_df)\n",
    "\n",
    "                test_df = pd.concat((test_t_df, test_d_df))\n",
    "                test_df_list.append(self._predict(test_df))\n",
    "        \n",
    "            return pd.concat(test_df_list, ignore_index=True)\n",
    "        else:\n",
    "\n",
    "            self._train(df_target, df_decoy)\n",
    "            test_df = pd.concat((df_target, df_decoy),ignore_index=True)\n",
    "        \n",
    "            return self._predict(test_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a3b27e141e49c996c9b863f8707e97aabd49c4a7e8445b9b783b34e4a21a9b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
